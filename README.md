# azure-ai-solution-engineer
## Contents of table
 - [Prepare to develop AI solutions on Azure](#prepare-to-develop-ai-solutions-on-azure)
 - [Create and consume Azure AI Services](#create-and-consume-azure-ai-services)
 - [Secure Azure AI Services](#secure-azure-ai-services)
 - [Monitor Azure AI Services](#monitor-azure-ai-services)
 - [Deploy Azure AI services in containers](#deploy-azure-ai-services-in-containers)
 - [Extract insights from text with the Azure AI Language service](#extract-insights-from-text-with-the-azure-ai-language-service)  
 - [Translate text with Azure AI Translator service](#translate-text-with-azure-ai-translator-service)
 - [Create speech enabled apps with Azure AI services](#create-speech-enabled-apps-with-azure-ai-services)
 - [Translate speech with the Azure AI Speech service](#translate-speech-with-the-azure-ai-speech-service)
 - [Build a question answering solution](#build-a-question-answering-solution)
 - [Create a bot with the Bot Framework SDK](#create-a-bot-with-the-bot-framework-sdk)
 - [Create a Bot with the Bot Framework Composer](#create-a-bot-with-the-bot-framework-composer)
 - [Analyze images](#analyze-images)
 - [Analyze video](#analyze-video)
 - [Classify images](#classify-images)
 - [Detect objects in images](#detect-objects-in-images)
 - [Detect analyze and recognize faces](#detect-analyze-and-recognize-faces)
 - [Read Text in images and documents with the Azure AI Vision Service](#read-text-in-images-and-documents-with-the-azure-ai-vision-service)
 - [Extract data from forms with Azure Document Intelligence](#extract-data-from-forms-with-azure-document-intelligence)
 - [Create an Azure AI Search solution](#create-an-azure-ai-search-solution)
 - [Create a custom skill for Azure AI Search](#create-a-custom-skill-for-azure-ai-search)
 - [Create a knowledge store with Azure AI Search](#create-a-knowledge-store-with-azure-ai-search)
 - [Enrich a search index using Language Studio](#enrich-a-search-index-using-language-studio)
 - [Implement advanced search features in Azure Cognitive Search](#implement-advanced-search-features-in-azure-cognitive-search)    
 - [Build an Azure Machine Learning custom skill for Azure Cognitive Search](#build-an-azure-machine-learning-custom-skill-for-azure-cognitive-search)
 - [Search data outside the Azure platform in Azure Cognitive Search using Azure Data Factory](#search-data-outside-the-azure-platform-in-azure-cognitive-search-using-azure-data-factory)
 - [Maintain an Azure Cognitive Search solution](#maintain-an-azure-cognitive-search-solution)


## Prepare to develop AI solutions on Azure
## Create and consume Azure AI Services
## Secure Azure AI Services
## Monitor Azure AI Services
## Deploy Azure AI services in containers
 - Containers enable you to host Azure AI Services either on-premises or on Azure.
 - A container is encapsulated in a container image that defines the software and configuration it must support. 
 - Understand containers: A container comprises an application or service and the runtime components needed to run it, while abstracting the underlying operating system and hardware.
   - Containers are portable across hosts
   - A single container host can support multiple isolated containers
   - Container deployment: A Docker* server, An Azure Container Instance (ACI), An Azure Kubernetes Service (AKS) cluster.
 - Use Azure AI Services containers
   - The container image for the specific Azure AI Services API you want to use is downloaded and deployed to a container host,
   - Client applications submit data to the endpoint provided by the containerized service, and retrieve results just as they would from an Azure AI Services cloud resource in Azure.
   - Periodically, usage metrics for the containerized service are sent to an Azure AI Services resource in Azure in order to calculate billing for the service.
   - **note:** Even when using a container, you must provision an Azure AI Services resource in Azure for billing purposes.(possible senitive data don't go to Azure, but container needs to connect to azure for billing purpose)
   - Azure AI Services container images: Each container provides a subset of Azure AI Services functionality.
   - Azure AI Services container configuration: ApiKey(Key from your deployed Azure AI Service; used for billing.), Billing(	Endpoint URI from your deployed Azure AI Service; used for billing.), Eula(Value of accept to state you accept the license for the container.)
   - Consuming Azure AI Services from a Container: adjust client apps such as endpoint, no need for subscription key, custom authentication solution, network security, etc
## Extract insights from text with the Azure AI Language service
 - Language detection: determining the language in which text is written.
   - keywords: **request: document(limit:5120 chars, 1000 items), item(countryHint, id, text). response: {language_name, language_code, confidence_score} -- multilingual content: predominant language , ambiguity: unknown & NaN**
 - Key phrase extraction: identifying important words and phrases in the text that indicate the main points.
   - limit: 5120 chars per document
 - Sentiment analysis: quantifying how positive or negative the text is. (scenarios: reviews)
   - keywords: **positive, negative, and neutral**
   - If all sentences are neutral, the overall sentiment is neutral.
   - If sentence classifications include only positive and neutral, the overall sentiment is positive.
   - If the sentence classifications include only negative and neutral, the overall sentiment is negative.
   - If the sentence classifications include positive and negative, the overall sentiment is mixed.
 - Named entity recognition - detecting references to entities, including people, locations, DateTime, organizations, addresses, email, URLs, and more.
 - Entity linking: identifying specific entities by providing reference links to Wikipedia articles.
## Translate text with Azure AI Translator service
 - Understand language detection, translation, and transliteration(translate it to a different language, you may want to transliterate it to a different script in order to get accurate pronounciation)
 - Specify translation options:
   - Word alignment: to understand the relationship between the characters in the source text and the corresponding characters in the translation. 
   - Sentence length: Sometimes it might be useful to know the length of a translation, for example to determine how best to display it in a user interface. You can get this information by setting the includeSentenceLength parameter to true.
   - Profanity filtering: Sometimes text contains profanities, which you might want to obscure or omit altogether in a translation. You can handle profanities by specifying the profanityAction parameter
     - NoAction
     - Deleted
     - Marked: Profanities are indicated using the technique indicated in the profanityMarker parameter (if supplied). The default value for this parameter is Asterisk, which replaces characters in profanities with "*". As an alternative, you can specify a profanityMarker value of Tag, which causes profanities to be enclosed in XML tags.
 - Define custom translations: While the default translation model used by Azure AI Translator is effective for general translation, you may need to develop a translation solution for businesses or industries in that have specific vocabularies of terms that require custom translation.
   - **note:** Your custom model is assigned a unique category Id (highlighted in the screenshot), which you can specify in translate calls to your Azure AI Translator resource by using the category parameter, causing translation to be performed by your custom model instead of the default model.
   - call custom translator API:
     - request parameters: `api-version`, `to`, `category`(your category ID)
     - request headers: `Ocp-Apim-Subscription-Key`, `Content-Type`
## Create speech-enabled apps with Azure AI services
 - Azure AI Speech provides APIs that you can use to build speech-enabled applications
   - Speech to text: An API that enables **speech recognition** in which your application can accept spoken input.
   - Text to speech: An API that enables **speech synthesis** in which your application can provide spoken output.
   - Speech Translation: An API that you can use to translate spoken input into multiple languages.
   - Speaker Recognition: An API that enables your application to recognize individual speakers based on their voice.
   - Intent Recognition: An API that uses conversational language understanding to determine the semantic meaning of spoken input.
 - Use the Azure AI Speech to text API:The Azure AI Speech service supports speech recognition through two REST APIs:
   - The Speech to text API, which is the primary way to perform speech recognition.
   - The Speech to text Short Audio API, which is optimized for short streams of audio (up to 60 seconds).
   - Using the Azure AI Speech SDK (consistent pattern):
     - SpeechConfig (encapsulate the information required to connect to your Azure AI Speech resource.)
     - AudioConfig (optional, define the input source for the audio to be transcribed. microphone or audio file)
     - SpeechRecognizer (SpeechConfig + AudioConfig): a proxy client for the Speech to text API.
     - use SpeechRecognizer to call the underlying API functions
     - Process the response from the Azure AI Speech service:
       - Duration
       - OffsetInTicks
       - Properties ('CancellationReason')
       - Reason ('RecognizedSpeech' | 'NoMatch' | 'Canceled')
       - ResultId
       - Text (the transcript)
 - Use the text to speech API:
   - The Text to speech API, which is the primary way to perform speech synthesis.
   - The Batch synthesis API, which is designed to support batch operations that convert large volumes of text to audio - for example to generate an audio-book from the source text.
   - Using the Azure AI Speech SDK (pattern):
     - SpeechConfig
     - AudioConfig (define the output device for the speech to be synthesized)
     - SpeechSynthesizer (SpeechConfig + AudioConfig): a proxy client for the Text to speech API.
     - use SpeechSynthesizer to call the underlying API functions
     - Process the response from the Azure AI Speech service:
       - AudioData
       - Properties
       - Reason ('SynthesizingAudioCompleted' | ...)
       - ResultId
 - Configure audio format and voices: **When synthesizing speech**, you can use a SpeechConfig object to customize the audio that is returned by the Azure AI Speech service.
   - Audio format:
     - Audio file type
     - Sample-rate
     - Bit-depth
     - To specify the required output format, use the **SetSpeechSynthesisOutputFormat** method of the **SpeechConfig** object
   - Voices:
     - Standard voices - synthetic voices created from audio samples.
     - Neural voices - more natural sounding voices created using deep neural networks.
     - To specify a voice for speech synthesis in the **SpeechConfig**, set its **SpeechSynthesisVoiceName** property to the voice you want to use
 - Use Speech Synthesis Markup Language:
   - Specify a speaking style, such as "excited" or "cheerful" when using a neural voice.
   - Insert pauses or silence.
   - Specify phonemes (phonetic pronunciations), for example to pronounce the text "SQL" as "sequel".
   - Adjust the prosody of the voice (affecting the pitch, timbre, and speaking rate).
   - Use common "say-as" rules, for example to specify that a given string should be expressed as a date, time, telephone number, or other form.
   - Insert recorded speech or audio, for example to include a standard recorded message or simulate background noise.
## Translate speech with the Azure AI Speech service
 - Translate speech to text:
   - SpeechTranslationConfig: encapsulate the information required to connect to your Azure AI Speech resource. Specifically, its location and key, also used to specify the speech recognition language (the language in which the input speech is spoken) and the target languages into which it should be translated.
   - AudioConfig(Optional): define the input source for the audio to be transcribed. By default, this is the default system microphone, but you can also specify an audio file.
   - TranslationRecognizer(SpeechTranslationConfig + AudioConfig): a proxy client for the Azure AI Speech translation API.
   - Use the methods of the TranslationRecognizer object to call the underlying API functions.
   - Process the response from Azure AI Speech. In the case of the RecognizeOnceAsync() method, the result is a SpeechRecognitionResult object that includes the following properties:
     - Duration
     - OffsetInTicks
     - Properties
     - Reason ('RecognizedSpeech'|'NoMatch'|'Cancelled')
     - ResultId
     - Text
     - Translations
 - Synthesize translations
   - Event-based synthesis: When you want to perform **1:1** translation (translating from one source language into a single target language), you can use event-based synthesis to capture the translation as an audio stream. Specify the desired voice for the translated speech in the TranslationConfig. Create an event handler for the TranslationRecognizer object's Synthesizing event. In the event handler, use the GetAudio() method of the Result parameter to retrieve the byte stream of translated audio.
   - Manual synthesis: essentially just the combination of two separate operations:
     - Use a TranslationRecognizer to translate spoken input into text transcriptions in one or more target languages.
     - Iterate through the Translations dictionary in the result of the translation operation, using a SpeechSynthesizer to synthesize an audio stream for each language.
## Build a question answering solution
 - Understand question answering: The Azure **AI Language** service includes a **question answering** capability, which enables you to define a **knowledge base** of question and answer pairs that can be queried using natural language input. The knowledge base can be published to a REST endpoint and consumed by client applications, commonly **bots**.
   - The knowledge base can be created from existing sources: FAQ website, Files containing structured text, Built-in chit chat question and answer pairs that encapsulate common conversational exchanges.
 - Compare question answering to Azure AI Language understanding: A question answering knowledge base is a form of language model, which raises the question of when to use question answering, and when to use the conversational language understanding capabilities of the Azure AI Language service. The two features are similar in that they both enable you to define a language model that can be queried using natural language expressions. The two services are in fact complementary. You can build comprehensive natural language solutions that combine conversational language understanding models and question answering knowledge bases.
   - Question answering: a static answer to a known question
   - conversational language understanding: Response indicates the most likely intent and referenced entities
 - Create a knowledge base:
   - Create an Azure AI Language resource in your Azure subscription( question answering feature, Create or select an Azure Cognitive Search resource to host the knowledge base index.).
   - In Azure AI Language Studio, select the Language resource and create a Custom question answering project (add data source, create knowledge base and edit Q&A pair).
 - Implement multi-turn conversation: sometimes you might need to ask follow-up questions to elicit more information from a user before presenting a definitive answer. This kind of interaction is referred to as a multi-turn conversation.
 - Test and publish a knowledge base: After you have defined a knowledge base, you can train its natural language model, and test it before publishing it for use in an application or bot.
 - Use a knowledge base: add a 'question' property in a request body
 - Improve question answering performance: After creating and testing a knowledge base, you can improve its performance with active learning and by defining synonyms.
   - Use active learning: Active learning can help you make continuous improvements so that it gets better at answering user questions correctly over time.
     - Implicit feedback: As incoming requests are processed, the service identifies user-provided questions that have multiple, similarly scored matches in the knowledge base. These are automatically clustered as alternate phrase suggestions for the possible answers that you can accept or reject in the Suggestions page for your knowledge base in Azure AI Language Studio.
     - Explicit feedback:
       - When developing a client application you can control the number of possible question matches returned for the user's input by specifying the **top** parameter. 
       - The response from the service includes a question object for each possible match, up to the **top** value specified in the request
       - You can implement logic in your client app to compare the score property values for the questions, and potentially present the questions to the user so they can positively identify the question closest to what they intended to ask.
       - With the correct question identified, your app can use the REST API to send feedback containing suggested alternative phrasing based on the user's original input.
       - The **qnaId** in the feedback corresponds to the **id** of the question the user identified as the correct match. The **userId** parameter is an identifier for the user and can be any value you choose, such as an email address or numeric identifier. The feedback will be presented in the active learning **Suggestions** page for your knowledge base in Azure AI Language Studio for you to accept or reject.
   - Define synonyms: Synonyms are useful when question submitted by users might include multiple different words to mean the same thing. To define synonyms, you must use the REST API to submit synonyms in the following JSON format.
 - Create a question answering bot: A bot is a conversational application that enables users to interact using natural language through one or more **channels**, such as email, web chat, voice messaging, or social media platform such as Microsoft Teams. To create a bot from your knowledge base, use Azure AI Language Studio to deploy the bot and then use the Create Bot button to create a bot in your Azure subscription. You can then edit and customize your bot in the Azure portal.
## Create a bot with the Bot Framework SDK
 - Introduce principles of bot design: Before embarking on the development of a bot, it's worth spending some time considering some principles for effective bot design.
   - Factors influencing a bot's success:
     - Is the bot discoverable? Discoverability can be achieved through integration with the proper channels. 
     - Is the bot intuitive and easy to use? The more difficult or frustrating a bot interaction is, the less use it will receive. Users will not return to a bad user experience.
     - Is the bot available on the devices and platforms that users care about? Knowing your customer-base is a good start to address this consideration. 
     - Can users solve their problems with minimal use and bot interaction? Although it may seem counter-intuitive, success doesn't equate to how long a user interacts with the bot. Users want answers to their issues or problems as quickly as possible. 
     - Does the bot solve the user issues better than alternative experiences? If a user can reach an answer with minimal effort through other means, they are less likely to use the bot.
   - Factors that do not guarantee success
     - Perhaps you want to ensure you have support for speech so that users don't have to type text for the interaction. Demonstrating factors such as these, may impress fellow developers, but are less likely to impress users. They could lead to user experience issues as well. The ability to support every language and dialect is not possible at this time. Speaker pronunciation and speed can greatly impact the accuracy. A user interacting with the bot in language that is not their native language can create issues in recognition. Other factors where speech enabled bots can be problematic are in noisy environments. Background noise will impact the accuracy of speech recognition and could create issues for the user in hearing the bot responses. Use voice only where it truly makes sense for bot user interaction.
     - Consider the concept of simplicity. The more complex your bot is, in terms of AI or machine learning features, the more open it may be to issues and problems. Consider adding advanced machine learning features to the bot if they are necessary to solve the problems the bot is designed to address.
     - Adding natural language features may not always make the bot experience great. A simple bot, that solves the user's problem without any conversational aspects, is still a successful bot.
   - Considerations for responsible AI
     - Articulate the purpose of your bot and take special care if your bot will support consequential use cases.
     - Be transparent about the fact that you use bots as part of your product or service.
     - Ensure a seamless hand-off to a human where the human-bot exchange leads to interactions that exceed the bot's competence.
     - Design your bot so that it respects relevant cultural norms and guards against misuse.
     - Ensure your bot is reliable.
     - Ensure your bot treats people fairly.
     - Ensure your bot respects user privacy.
     - Ensure your bot handles data securely.
     - Ensure your bot is accessible.
     - Accept responsibility for your bots operation and how it affects people.
 - Get started with the Bot Framework SDK:
   - Bot solutions on Microsoft Azure are supported by the following technologies:
     - Azure AI Bot Service. A cloud service that enables bot delivery through one or more channels, and integration with other services.
     - Bot Framework Service. A component of Azure AI Bot Service that provides a REST API for handling bot activities.
     - Bot Framework SDK. A set of tools and libraries for end-to-end bot development that abstracts the REST interface, enabling bot development in a range of programming languages.
   - Developing a Bot with the Bot Framework SDK: The Bot Framework SDK provides an extensive set of tools and libraries that software engineers can use to develop bots.
     - Bot templates
       - Empty Bot - a basic bot skeleton.
       - Echo Bot - a simple "hello world" sample in which the bot responds to messages by echoing the message text back to the user.
       - Core Bot - a more comprehensive bot that includes common bot functionality, such as integration with the Language Understanding service.
     - Bot application classes and logic
       - The template bots are based on the **Bot** class defined in the Bot Framework SDK, which is used to implement the logic in your bot that receives and interprets user input, and responds appropriately. Additionally, bots make use of an **Adapter** class that handles communication with the user's channel. Conversations in a bot are composed of **activities**, which represent events such as a user joining a conversation or a message being received. These activities occur within the context of a **turn**, a two-way exchange between the user and bot. The Bot Framework Service notifies your bot's adapter when an activity occurs in a channel by calling its **Process Activity** method, and the adapter creates a context for the turn and calls the bot's **Turn Handler** method to invoke the appropriate logic for the activity. 
     - Testing with the Bot Framework Emulator
       - Bots developed with the Bot Framework SDK are designed to run as cloud services in Azure, but while developing your bot, you'll need a way to test it before you deploy it into production. The Bot Framework Emulator is an application that enables you to run your bot local or remote web applications and connect to it from an interactive web chat interface that you can use to test your bot. Details of activity events are captured and shown in the testing interface, so you can monitor your bots behavior as you submit messages and review the responses.
 - Implement activity handlers and dialogs: The logic for processing the activity can be implemented in multiple ways. The Bot Framework SDK provides classes that can help you build bots that manage conversations using
   - Activity handlers: Event methods that you can override to handle different kinds of activities.
     - For simple bots with short, stateless interactions, you can use Activity Handlers to implement an event-driven conversation model in which the events are triggered by activities such as users joining the conversation or a message being received. When an activity occurs in a channel, the Bot Framework Service calls the bot adapter's **Process Activity** function, passing the activity details. The adapter creates a turn context for the activity and passes it to the bot's turn handler, which calls the individual, event-specific activity handler. The **ActivityHandler** base class includes event methods for the many kinds of common activity, including: Message received, Members left the conversation...
     - Turn context: An activity occurs within the context of a **turn**, which represents a single two-way exchange between the user and the bot. Activity handler methods include a parameter for the **turn context**, which you can use to access relevant information. For example, the activity handler for a message received activity includes the text of the message.
   - Dialogs: For more complex conversational flows where you need to store **state** between turns to enable a **multi-turn conversation**, you can implement **dialogs**. The Bot Framework SDK dialogs library provides multiple dialog classes that you can combine to implement the required conversational flow for your bot.
     - Component dialogs: A **component** dialog is a dialog that can contain other dialogs, defined in its **dialog set**. Often, the initial dialog in the component dialog is a **waterfall** dialog, which defines a sequential series of steps to guide the conversation. It's common for each step to be a **prompt** dialog so that conversational flow consists of gathering input data from the user sequentially. Each step must be completed before passing the output onto the next step
     - Adaptive dialogs: An **adaptive** dialog is another kind of container dialog in which the flow is more flexible, allowing for interruptions, cancellations, and context switches at any point in the conversation. In this style of conversation, the bot initiates a **root** dialog, which contains a flow of **actions** (which can include branches and loops), and **triggers** that can be initiated by actions or by a **recognizer**. The recognizer analyzes natural language input (usually using the Language Understanding service) and detects intents, which can be mapped to triggers that change the flow of the conversation - often by starting new child dialogs, which contain their own actions, triggers, and recognizers.
 - Deploy a bot
   - Create the Azure resources required to support your bot:
     - Register an Azure app: You can create the application registration by using the az ad app create Azure command-line interface (CLI) command, specifying a display name and password for your app identity. This command registers the app and returns its registration information, including a unique application ID that you will need in the following step.
     - Create a bot application service: Your bot requires a **Bot Channels Registration** resource, along with associated application service and application service plan. To create these resources, you can use the Azure resource deployment templates provided with the Bot Framework SDK template you used to create your bot. Just run the az deployment group create command, referencing the deployment template and specifying your bot application registration's ID (from the az ad app create command output) and the password you specified.
   - Prepare your bot for deployment: The specific steps you need to perform to prepare your bot depend on the programming language used to create it. For C# and JavaScript bots, you can use the az bot prepare-deploy command to ensure your bot is properly configured with the appropriate package dependencies and build files. For Python bots, you must include a requirements.txt file listing any package dependencies that must be installed in the deployment environment.
   - Deploy your bot as a web app: The final step is to package your bot application files in a zip archive, and use the az webapp deployment source config-zip command to deploy the bot code to the Azure resources you created previously. After deployment has completed, you can test and configure your bot in the Azure portal.
## Create a Bot with the Bot Framework Composer
 - Understand ways to build a bot:
   - Power Virtual Agents: Power Virtual Agents (PVA) is built on the Microsoft Power Platform, and enables users to build a chatbot without requiring any code. PVA lets users use an interface to build conversations, send messages, publish, monitor, and configure your bot all within the PVA app. This PVA app is tailored for individuals who prefer not to write any code and use a graphical interface to build their bot, or for teams made up of both subject matter experts and developers working together. Some features of the Azure AI Bot Service aren't available in the PVA app, and require using Bot Framework Composer (which can be launched directly from the PVA web app) to integrate those features.
   - Bot Framework Composer: Bot Framework Composer is an app for developers to build, test, and publish your bot via an interactive interface. Composer is built on the Bot Framework SDK, and supports extending your bot with code for more complex interactions. Composer enables use of all of the Azure AI Bot Service features. Composer is best for developers who want to use both a visual interface and code to author a bot.
   - Bot Framework SDK: The Bot Framework SDK is a collection of libraries and tools to build, test, publish, and manage conversational bots. The SDK can connect to other AI services, covers end-to-end bot development, and offers the most authoring flexibility. Developers can use their favorite development environment to build and enhance their bot, and then connect the bot to various channels through the Azure AI Bot Service.
 - Get started with the Bot Framework Composer:
   - Visual design surface in Composer eliminates the need for boilerplate code and makes bot development more accessible.
   - Save time with fewer steps to set up your environment.
   - Visualize Dialogs, allow for building portions of the bot's functionality and how to guide the conversation.
   - Triggers are easily created to invoke a specific dialog, and enabling interruptions is handled by switching a value within a prompt.
   - Composer enables saving of pieces of data to various scopes to remember things between dialogs or sessions.
   - Test your bot directly inside Composer via embedded Web Chat.
 - Understand dialogs: In all but the most simple cases, your bot will likely make use multiple dialogs to implement multi-turn conversations in which the bot gathers information from the user, storing state between turns. Commonly, a bot interaction begins with a main dialog in which the bot welcomes a user and establishes the initial conversation, and then triggers child dialogs.
   - A flow of dialogs: The important thing is to consider the purpose of your bot - what should it help the user achieve? Then design a conversation flow based on dialogs that will gather the required information and get to a resolution efficiently.
   - Implementing dialogs with the Bot Framework Composer: The Bot Framework Composer implements dialogs (previously called adaptive dialogs, which they're still sometimes referred to) to build the bot conversation. Dialogs have a flexible conversation flow, allowing for interruptions, cancellations, and context switches at any point in the conversation. Each dialog consists of:
     - One or more actions that define the flow of message activities in the dialog. These include sending a message, prompting the user for input, asking a question, and branching the conversation.
     - A Trigger, which invokes the dialog logic for certain conditions or based on intent detected.
     - A recognizer, which interprets user input to determine semantic intent. Recognizers are based on the Language Understanding service by default, but you can also use other types of recognizer; such as the QnA Service or simple regular expression matches.
     - A message or response for your bot can have one or more responses to choose from, which are chosen by random for a more dynamic conversation.
     - In addition to these elements, a dialog has **memory** in which values are stored as properties. Properties can be defined at various scopes, including the user scope (variables that store information for the lifetime of the user session with the bot, such as user.greeted) and dialog scope (variables that persist for the lifetime of the dialog, such as dialog.response).
 - Understand adaptive flow: The Bot Framework Composer makes use of dialogs that can handle unexpected input as an interruption to the programmed flow of the conversation. A better design is to implement an adaptive dialog that enables you to handle the interruption and redirect the flow of the conversation. The dialog can maintain state so that relevant information that has already been gathered can be retained to pick up where it left off; or in some cases, restart the dialog (or the entire conversation), resetting state as appropriate.
   - Managing interruptions with the Bot Framework Composer: When using the Bot Framework Composer, user input is provided through actions in a dialog flow, which can be configured to allow interruptions. An interruption occurs when the recognizer identifies input that fires a trigger, signaling a conversational context change - usually by ending the current dialog flow or starting a child dialog. For example, a trigger might respond to the entry of the term "cancel" by ending the current dialog flow and resetting all dialog-scope variables. The ability to handle interruptions is configurable for each user input action, under the **Prompt Configurations** tab of the action.
 - Design the user experience: An important consideration for the user experience is how you present the bot and its components to the user. You can implement the following features into a bot: 1)Text - a typical interaction that is lightweight and involves presenting text to the user and having the user respond with text input. 2)Buttons - presenting the user with buttons from which to select options. In a pizza order bot, you might decide to use buttons to represent the pizza sizes available. They are a visual way to represent choices to users and add more visual appeal when compared to text. 3)Images - using images in the bot interaction adds a graphical appearance to the bot and can enhance the user experience. 4)Cards - allow you to present your users with various visual, audio, and/or selectable messages and help to assist conversation flow.
   - Text: Text input from users is parsed to determine the intent. The ability to add natural language understanding to a bot is possible. Careful consideration around language understanding is important. One of the main reasons concerns how different users will respond to a question. Careful planning could reveal a better design option where the bot is specific in the prompt. Your bot can integrate different Azure AI services to aid in language understanding, keyword, or phrase detection, and sentiment analysis. These features make your bot more "intelligent" but they also lead to response time delays if too many services are integrated for each response. Essentially, the less processing required on the user input, the less chance for misinterpretation or bot performance. The following are recommended considerations for text input, from Microsoft.
     - Whenever possible, ask specific questions that do not require natural language understanding capabilities to parse the response. It will simplify your bot and increase the success with which your bot understands the user
     - Designing a bot to require specific commands from the user can often provide a good user experience while also eliminating the need for natural language understanding capability.
     - If you are designing a bot that will answer questions based on structured or unstructured data from databases, web pages, or documents, consider using technologies like QnA Maker that are designed specifically to address this scenario.
     - When building natural language models, do not assume that users will provide all the required information in their initial query. Design your bot to specifically request the information it requires, guiding the user to provide that information by asking a series of questions, if necessary.
   - Speech: You can design your bot to take advantage of speech input and output. You may decide that your bot application needs to support speech if it will be accessed from devices that do not contain keyboards or monitors. You may also design your bot for users with differing abilities to interact with computing devices. Using speech will require your bot to interact with the Azure AI Speech to transcribe the spoken word to text, for actions by the bot, and then synthesize the text responses to speech as the output.
   - Rich user controls: Buttons, images, carousels, and menus are examples of rich user controls. The advantage to using these types of controls with your bot are:
     - Provide a more guided experience with the bot.
     - Emulate an application. Users are familiar with using applications on their computers or devices so it makes the bot use more "natural".
     - Presents the user with discrete choices resulting in less ambiguity and misinterpretation by the bot's logic.
     - Ease of use on mobile devices where typing text is not optimal or less-preferred by users.
   - Cards: Cards allow you to present your users with various visual, audio, and/or selectable messages and help to assist conversation flow. Cards are programmable objects containing standardized collections of rich user controls. An advantage of cards is that they are recognized across a wide range of channels.
     - Adaptive cards: An open card exchange format rendered as a JSON object. Typically used for cross-channel deployment of cards. Cards adapt to the look and feel of each host channel.
     - Audio cards: A card that can play audio files. This card could be helpful in a bot that interacts with users who have visual impairments.
     - Animation cards: This type of card can play animated GIFs or short video files, for example to depict actions or status indicators.
     - Hero cards: A card that contains a single large image, one or more buttons, and text. Typically used to visually highlight a potential user selection.
     - Thumbnail cards: A card that contains a single thumbnail image, one or more buttons, and text. Typically used to visually highlight the buttons for a potential user selection.
     - SignIn card: A card that enables a bot to request that a user sign-in. It typically contains text and one or more buttons that the user can select to initiate the sign-in process.
     - Video card: A card that can play videos. Typically used to open a URL and stream an available video.
   - Recommendations for choosing the experience options
     - Pizza Order Bot: Text, Adaptive card, Hero card
     - Flight Booking: Text, Thumbnail card, Adaptive card
     - Sporting Events: Hero card, Adaptive card, Video card
   - Presenting responses with the Bot Framework Composer: All responses that your bot presents to users are listed for the current (or parent) dialog. At the root of the bot, it uses what's called **language generator** (lg) to create the responses. You use templates to define responses (viewed by clicking `Show code` for the selected dialog within Composer), which can include multiple phrases for a given type of response, or specific graphical responses.
## Analyze images
 - Provision an Azure AI Vision resource: The Azure AI Vision service is designed to help you extract information from images. It provides functionality that you can use for:
   - Description and tag generation
   - Object detection
   - People detection
   - Image metadata, color, and type analysis
   - Category identification
   - Background removal
   - Moderation rating
   - Optical character recognition
   - Smart thumbnail generation
 - Analyze an image: To analyze an image, you can use the **Analyze Image** REST method or the equivalent method in the SDK for your preferred programming language, specifying the visual features you want to include in the analysis (and if you select categories, whether or not to include details of celebrities or landmarks). This method returns a JSON document containing the requested information.
 - Generate a smart-cropped thumbnail: Thumbnails are often used to provide smaller versions of images in applications and websites. The Azure AI Vision service enables you to create a thumbnail with different dimensions (and aspect ratio) from the source image, and optionally to use image analysis to determine the region of interest in the image (its main subject) and make that the focus of the thumbnail. This ability to determine the region of interest is especially useful when cropping the image to change its aspect ratio.
## Analyze video
 - Understand Azure Video Indexer capabilities: The Azure Video Indexer service is designed to help you extract information from videos. It provides functionality that you can use for:
   - Facial recognition - detecting the presence of individual people in the image. This requires Limited Access approval.
   - Optical character recognition - reading text in the video.
   - Speech transcription - creating a text transcript of spoken dialog in the video.
   - Topics - identification of key topics discussed in the video.
   - Sentiment - analysis of how positive or negative segments within the video are.
   - Labels - label tags that identify key objects or themes throughout the video.
   - Content moderation - detection of adult or violent themes in the video.
   - Scene segmentation - a breakdown of the video into its constituent scenes.
 - Extract custom insights: Azure Video Indexer includes predefined models that can recognize well-known celebrities, do OCR, and transcribe spoken phrases into text. You can extend the recognition capabilities of Video Analyzer by creating custom models for:
   - People. Add images of the faces of people you want to recognize in videos, and train a model. Video Indexer will then recognize these people in all of your videos.
   - Language. If your organization uses specific terminology that may not be in common usage, you can train a custom model to detect and transcribe it.
   - Brands. You can train a model to recognize specific names as brands, for example to identify products, projects, or companies that are relevant to your business.
 - Use Video Analyzer widgets and APIs: While you can perform all video analysis tasks in the Azure Video Indexer portal, you may want to incorporate the service into custom applications. There are two ways you can accomplish this:
   - Azure Video Indexer widgets: The widgets used in the Azure Video Indexer portal to play, analyze, and edit videos can be embedded in your own custom HTML interfaces. You can use this technique to share insights from specific videos with others without giving them full access to your account in the Azure Video Indexer portal.
   - Azure Video Indexer API: Azure Video Indexer provides a REST API that you can use to obtain information about your account, including an access token.
## Classify images
 - Provision Azure resources for Azure AI Custom Vision: The Azure AI Custom Vision service enables you to `build your own computer vision models` for **image classification** or **object detection**.
   - Creating an Azure AI Custom Vision solution involves two tasks:
     - Use existing (labeled) images to train an Azure AI Custom Vision model.
     - Create a client application that submits new images to your model to generate predictions.
   - To use the Azure AI Custom Vision service, you must provision two kinds of Azure resource:
     - A **training resource** (used to train your models). This can be: **An Azure AI Services resource**./**An Azure AI Custom Vision (Training) resource**.
     - A **prediction resource**, used by client applications to get predictions from your model. This can be: **An Azure AI Services resource.**/**An Azure AI Custom Vision (Prediction) resource.**
   - You can use a single **Azure AI Services** resource for both training and prediction, and you can mix-and-match resource types (for example, using an **Azure AI Custom Vision (Training) resource** to train a model that you then publish using an **Azure AI Services resource**).
 - Understand image classification: Image classification is a computer vision technique in which a model is trained to predict a class label for an image based on its contents. Usually, the class label relates to the main subject of the image. Models can be trained for multiclass classification (in other words, there are multiple classes, but each image can belong to only one class) or multilabel classification (in other words, an image might be associated with multiple labels).
 - Train an image classifier: To train an image classification model with the Azure AI Custom Vision service, you can use the Azure AI Custom Vision portal, the Azure AI Custom Vision REST API or SDK, or a combination of both approaches.
   - In most cases, you'll typically use the Azure AI Custom Vision portal to train your model.
     - Create an image classification project for your model and associate it with a training resource.
     - Upload images, assigning class label tags to them.
     - Review and edit tagged images.
     - Train and evaluate a classification model.
     - Test a trained model.
     - Publish a trained model to a prediction resource.
   - **The REST API and SDKs enable you to perform the same tasks by writing code, which is useful if you need to automate model training and publishing as part of a DevOps process.**
## Detect objects in images
 - Understand object detection: Object detection is a form of computer vision in which a model is trained to detect the presence and location of one or more classes of object in an image. There are two components to an object detection prediction: The class label of each object detected in the image. The location of each object within the image, indicated as coordinates of a **bounding box** that encloses the object.
   - You can use the Azure AI Custom Vision service to train an object detection model. To use the Azure AI Custom Vision service, you must provision two kinds of Azure resource:
     - A training resource (used to train your models). This can be: An Azure AI Services resource./ An Azure AI Custom Vision (Training) resource.
     - A prediction resource, used by client applications to get predictions from your model. This can be: An Azure AI Services resource. / An Azure AI Custom Vision (Prediction) resource.
   - You can use a single Azure AI Services resource for both training and prediction, and you can mix-and-match resource types (for example, using an Azure AI Custom Vision (Training) resource to train a model that you then publish using an Azure AI Services resource).
 - Train an object detector: The most significant difference between training an image classification model and training an object detection model is the labeling of the images with tags. While image classification requires one or more tags that apply to the whole image, object detection requires that each label consists of a tag and a region that defines the bounding box for each object in an image. The Azure AI Custom Vision portal provides a graphical interface that you can use to label your training images.
 - Consider options for labeling images: The easiest option for labeling images for object detection is to use the interactive interface in the Azure AI Custom Vision portal. This interface automatically suggests regions that contain objects, to which you can assign tags or adjust by dragging the bounding box to enclose the object you want to label. Additionally, after tagging an initial batch of images, you can train the model. Subsequent labeling of new images can benefit from the **smart labeler tool** in the portal, which can suggest not only the regions, but the classes of object they contain. Alternatively, you can use a labeling tool, such as the one provided in **Azure Machine Learning Studio** or the Microsoft **Visual Object Tagging Tool (VOTT)**, to take advantage of other features, such as assigning image labeling tasks to multiple team members.
   - Bounding box measurement units: If you choose to use a labeling tool other than the Azure AI Custom Vision portal, you may need to adjust the output to match the measurement units expected by the Azure AI Custom Vision API. Bounding boxes are defined by four values that represent the left (X) and top (Y) coordinates of the top-left corner of the bounding box, and the width and height of the bounding box. These values are expressed as **proportional** values relative to the source image size. 

## Detect, analyze, and recognize faces
 - Identify options for face detection analysis and identification
   - There are two Azure AI services that you can use to build solutions that detect faces or people in images.
     - The Azure AI Vision service:  enables you to detect people in an image, as well as returning a bounding box for its location.
     - The Face service:
       - Face detection (with bounding box).
       - Comprehensive facial feature analysis (including head pose, presence of spectacles, blur, facial landmarks, occlusion and others).
       - Face comparison and verification.
       - Facial recognition.
 - Understand considerations for face analysis
   - When building a solution that uses facial data, considerations include (but are not limited to):
     - Data privacy and security. Facial data is personally identifiable, and should be considered sensitive and private. You should ensure that you have implemented adequate protection for facial data used for model training and inferencing.
     - Transparency. Ensure that users are informed about how their facial data will be used, and who will have access to it.
     - Fairness and inclusiveness. Ensure that you face-based system cannot be used in a manner that is prejudicial to individuals based on their appearance, or to unfairly target individuals.
 - Detect faces with the Azure AI Vision service
   - To detect and analyze faces with the Azure AI Vision service, call the **Analyze Image** function (SDK or equivalent REST method), specifying People as one of the visual features to be returned.
 - Understand capabilities of the face service: The Face service provides comprehensive facial detection, analysis, and recognition capabilities.
   - The Face service provides functionality that you can use for:
     - Face detection - for each detected face, the results include an ID that identifies the face and the bounding box coordinates indicating its location in the image.
     - Face attribute analysis - you can return a wide range of facial attributes, including:
       - Head pose (pitch, roll, and yaw orientation in 3D space)
       - Glasses (NoGlasses, ReadingGlasses, Sunglasses, or Swimming Goggles)
       - Blur (low, medium, or high)
       - Exposure (underExposure, goodExposure, or overExposure)
       - Noise (visual noise in the image)
       - Occlusion (objects obscuring the face)
       - Accessories (glasses, headwear, mask)
       - QualityForRecognition (low, medium, or high)
     - Facial landmark location - coordinates for key landmarks in relation to facial features (for example, eye corners, pupils, tip of nose, and so on)
     - Face comparison - you can compare faces across multiple images for similarity (to find individuals with similar facial features) and verification (to determine that a face in one image is the same person as a face in another image)
     - Facial recognition - you can train a model with a collection of faces belonging to specific individuals, and use the model to identify those people in new images.
     - Facial liveness - liveness can be used to determine if the input video is a real stream or a fake to prevent bad intentioned individuals from spoofing the recognition system.
 - Compare and match detected faces: When a face is detected by the Face service, an ID is assigned to it and retained in the service resource for 24 hours. The ID is a GUID, with no indication of the individual's identity other than their facial features.
   - While the detected face ID is cached, subsequent images can be used to compare the new faces to the cached identity and determine if they are similar (in other words, they share similar facial features) or to verify that the same person appears in two images.
   - This ability to compare faces anonymously can be useful in systems where it's important to confirm that the same person is present on two occasions, without the need to know the actual identity of the person. For example, by taking images of people as they enter and leave a secured space to verify that everyone who entered leaves.
 - Implement facial recognition
   - For scenarios where you need to positively identify individuals, you can train a facial recognition model using face images.
   - To train a facial recognition model with the Face service:
     - Create a `Person Group` that defines the set of individuals you want to identify (for example, employees).
     - Add a `Person` to the Person Group for each individual you want to identify.
     - Add detected faces from multiple images to each person, preferably in various poses. The IDs of these faces will no longer expire after 24 hours (so they're now referred to as `persisted faces`).
     - Train the model.
   - The trained model is stored in your Face (or Azure AI Services) resource, and can be used by client applications to:
     - Identify individuals in images.
     - Verify the identity of a detected face.
     - Analyze new images to find faces that are similar to a known, `persisted` face.
## Read Text in images and documents with the Azure AI Vision Service
 - Explore Azure AI Vision options for reading text: Azure AI provides two different features that read text from documents and images, one in the Azure AI Vision Service, the other in Azure AI Document Intelligence. There is overlap in what each service provides, however each is optimized for results depending on what the input is.
   - Image Analysis Optical character recognition (OCR):
     - Use this feature for general, unstructured documents with smaller amount of text, or images that contain text.
     - Results are returned immediately (synchronous) from a single API call.
     - Has functionality for analyzing images past extracting text, including object detection, describing or categorizing an image, generating smart-cropped thumbnails and more.
     - Examples include: street signs, handwritten notes, and store signs.
   - Document Intelligence:
     - Use this service to read small to large volumes of text from images and PDF documents.
     - This service uses context and structure of the document to improve accuracy.
     - The initial function call returns an asynchronous operation ID, which must be used in a subsequent call to retrieve the results.
     - Examples include: receipts, articles, and invoices.
 - Use the Read API
   - To use the Read OCR feature, call the ImageAnalysis function (REST API or equivalent SDK method), passing the image URL or binary data, and optionally specifying a gender neutral caption or the language the text is written in (with a default value of en for English).
   - To make an OCR request to ImageAnalysis, specify the analysis features as TEXT.
   - The results of the Read OCR function are returned synchronously, either as JSON or the language specific object of a similar structure. These results are provided as a complete result and broken down by `page, then words, and then lines`. Additionally, the text values are included at both the `line` and `word` levels, making it easier to read entire lines of text if you don't need to extract text at the individual `word` level.
## Extract data from forms with Azure Document Intelligence
 - What is Azure Document Intelligence? Azure Document Intelligence is one of many Azure AI Services, cloud-based artificial intelligence (AI) services with REST APIs and client library SDKs that can be used to build intelligence into your applications. Azure Document Intelligence uses Optical Character Recognition (OCR) capabilities and deep learning models to extract text, key-value pairs, selection marks, and tables from documents. OCR captures document structure by creating bounding boxes around detected objects in an image. The locations of the bounding boxes are recorded as coordinates in relation to the rest of the page. Azure Document Intelligence services return bounding box data and other information in a structured form with the relationships from the original file. To build a high-accuracy model from scratch, people need to build deep learning models, use a large amount of compute resources, and face long model training times. These factors could make a project infeasible. Azure Document Intelligence provides underlying models that have been trained on thousands of form examples. The underlying models enable you to do high-accuracy data extraction from your forms with little to no model training.
   - Azure Document Intelligence service components:
     - Document analysis models: which take an input of JPEG, PNG, PDF, and TIFF files and return a JSON file with the location of text in bounding boxes, text content, tables, selection marks (also known as checkboxes or radio buttons), and document structure.
     - Prebuilt models: which detect and extract information from document images and return the extracted data in a structured JSON output. Azure Document Intelligence currently supports prebuilt models for several forms, including: W-2 forms, Invoices, Receipts, ID documents, Business cards.
     - Custom models: custom models extract data from forms specific to your business. Custom models can be trained through the Azure Document Intelligence Studio.
 - Get started with Azure Document Intelligence
   - Subscribe to a resource: A Azure AI Service resource / A Azure Document Intelligence resource
   - Understand Azure Document Intelligence file input requirements:
     - Format must be JPG, PNG, BMP, PDF (text or scanned), or TIFF.
     - The file size must be less than 500 MB for paid (S0) tier and 4 MB for free (F0) tier.
     - Image dimensions must be between 50 x 50 pixels and 10000 x 10000 pixels.
     - The total size of the training data set must be 500 pages or less.
   - Decide what component of Azure Document Intelligence to use: After you have collected your files, decide what you need to accomplish.
     - To use OCR capabilities to capture document analysis, use the Layout model, Read model, or General Document model.
     - To create an application that extracts data from W-2s, Invoices, Receipts, ID documents, Health insurance, vaccination, and business cards, use a prebuilt model. These models do not need to be trained. Azure Document Intelligence services analyze the documents and return a JSON output.
     - To create an application to extract data from your industry-specific forms, create a custom model. This model needs to be trained on sample documents. After training, the custom model can analyze new documents and return a JSON output.
 - Train custom models: Azure's Azure Document Intelligence service supports supervised machine learning. You can train custom models and create composite models with form documents and JSON documents that contain labeled fields. To train a custom model:
   - Store sample forms in an Azure blob container, along with JSON files containing layout and label field information
     - You can generate an ocr.json file for each sample form using the Azure Document Intelligence's Analyze document function. Additionally, you need a single fields.json file describing the fields you want to extract, and a labels.json file for each sample form mapping the fields to their location in that form.
   - Generate a shared access security (SAS) URL for the container.
   - Use the Build model REST API function (or equivalent SDK method).
   - Use the Get model REST API function (or equivalent SDK method) to get the trained model ID.
   - **Or**, Use the Azure Document Intelligence Studio to label and train. There are two types of underlying models for custom forms custom template models or custom neural models.
     - Custom template models accurately extract labeled key-value pairs, selection marks, tables, regions, and signatures from documents. Training only takes a few minutes, and more than 100 languages are supported.
     - Custom neural models are deep learned models that combine layout and language features to accurately extract labeled fields from documents.This model is best for **semi-structured or unstructured documents**.
 - Use Azure Document Intelligence models
   - Using the API: To extract form data using a custom model, use the **analyze document** function of either a supported SDK, or the REST API, while supplying model ID (generated during model training). This function starts the form analysis. which you can then request the result to get the analysis. A successful JSON response contains **analyzeResult** that contains the content extracted and an array of pages containing information about the document content.
   - Understanding confidence scores
     - If the confidence values of the analyzeResult are low, try to improve the quality of your input documents. Depending on the use case, you might find that a confidence score of 80% or higher is acceptable for a low-risk application. For more sensitive cases, like reading medical records or billing statements, a score of 100% is recommended.
 - Use the Azure Document Intelligence Studio
   - The Azure Document Intelligence Studio currently supports the following projects:
     - Document analysis models:
       - Read: Extract printed and handwritten text lines, words, locations, and detected languages from documents and images.
       - Layout: Extract text, tables, selection marks, and structure information from documents (PDF and TIFF) and images (JPG, PNG, and BMP).
       - General Documents: Extract key-value pairs, selection marks, and entities from documents.
     - Prebuilt models: To extract data from common forms(W-2s, Invoices, Receipts, ID documents, Health insurance, vaccination, and business cards.) with prebuilt models
     - Custom models: When you use Azure Document Intelligence Studio to build custom models, **the ocr.json files, labels.json files, and fields.json** file needed for training are automatically created and stored in your storage account.
## Create an Azure AI Search solution
 - Manage capacity: To create an Azure AI Search solution, you need to create an Azure AI Search resource in your Azure subscription. Depending on the specific solution you intend to build, you may also need Azure resources for data storage and other application services.
   - Service tiers and capacity management: When you create an Azure AI Search resource, you must specify a `pricing tier`. The pricing tier you select determines the capacity limitations of your search service and the configuration options available to you, as well as the cost of the service. The available pricing tiers are:
     - Free (F). Use this tier to explore the service or try the tutorials in the product documentation.
     - Basic (B): Use this tier for small-scale search solutions that include a maximum of 15 indexes and 2 GB of index data.
     - Standard (S): Use this tier for enterprise-scale solutions. There are multiple variants of this tier, including S, S2, and S3; which offer increasing capacity in terms of indexes and storage, and S3HD, which is optimized for fast read performance on smaller numbers of indexes.
     - Storage Optimized (L): Use a storage-optimized tier (L1 or L2) when you need to create large indexes, at the cost of higher query latency.
   - Replicas and partitions: Depending on the pricing tier you select, you can optimize your solution for scalability and availability by creating replicas and partitions. The combination of replicas and partitions you configure determines the search units used by your solution. Put simply, the number of search units is the number of replicas multiplied by the number of partitions (R x P = SU). For example, a resource with four replicas and three partitions is using 12 search units.
     - `Replicas` are instances of the search service - you can think of them as nodes in a cluster. Increasing the number of replicas can help ensure there is sufficient capacity to service multiple concurrent query requests while managing ongoing indexing operations.
     - `Partitions` are used to divide an index into multiple storage locations, enabling you to split I/O operations such as querying or rebuilding an index.
 - Understand search components
   - Data source: Azure AI Search supports multiple types of data source, including:
     - Unstructured files in Azure blob storage containers.
     - Tables in Azure SQL Database.
     - Documents in Cosmos DB.
     - `Alternatively, applications can push JSON data directly into an index, without pulling it from an existing data store.`
   - Skillset: In Azure AI Search, you can apply artificial intelligence (AI) skills as part of the indexing process to enrich the source data with new information, which can be mapped to index fields. The skills used by an indexer are encapsulated in a skillset that defines an enrichment pipeline in which each step enhances the source data with insights obtained by a specific AI skill. Examples of the kind of information that can be extracted by an AI skill include:
     - The language in which a document is written.
     - Key phrases that might help determine the main themes or topics discussed in a document.
     - A sentiment score that quantifies how positive or negative a document is.
     - Specific locations, people, organizations, or landmarks mentioned in the content.
     - AI-generated descriptions of images, or image text extracted by optical character recognition.
     - Custom skills that you develop to meet specific requirements.
   - Indexer: The indexer is the engine that drives the overall indexing process. It takes the outputs extracted using the skills in the skillset, along with the data and metadata values extracted from the original data source, and maps them to fields in the index. An indexer is automatically run when it is created, and can be scheduled to run at regular intervals or run on demand to add more documents to the index. In some cases, such as when you add new fields to an index or new skills to a skillset, you may need to reset the index before re-running the indexer.
   - Index: The index is the searchable result of the indexing process. It consists of a collection of JSON documents, with fields that contain the values extracted during indexing. Client applications can query the index to retrieve, filter, and sort information. Each index field can be configured with the following attributes:
     - key: Fields that define a unique key for index records.
     - searchable: Fields that can be queried using full-text search.
     - filterable: Fields that can be included in filter expressions to return only documents that match specified constraints.
     - sortable: Fields that can be used to order the results.
     - facetable: Fields that can be used to determine values for facets (user interface elements used to filter the results based on a list of known field values).
     - retrievable: Fields that can be included in search results (by default, all fields are retrievable unless this attribute is explicitly removed).
 - Understand the indexing process: The indexing process works by creating a `document` for each indexed entity. During indexing, an enrichment pipeline iteratively builds the documents that combine metadata from the data source with enriched fields extracted by cognitive skills. You can think of each indexed document as a JSON structure, which initially consists of a `document` with the index fields you have mapped to fields extracted directly from the source data. When the documents in the data source contain images, you can configure the indexer to extract the image data and place each image in a `normalized_images` collection. Normalizing the image data in this way enables you to use the collection of images as an input for skills that extract information from image data. Each skill adds fields to the document, so for example a skill that detects the `language` in which a document is written might store its output in a `language` field. The document is structured hierarchically, and the skills are applied to a specific `context` within the hierarchy, enabling you to run the skill for each item at a particular level of the document. The output fields from each skill can be used as inputs for other skills later in the pipeline, which in turn store their outputs in the document structure. The fields in the final document structure at the end of the pipeline are mapped to index fields by the indexer in one of two ways:
   - Fields extracted directly from the source data are all mapped to index fields. These mappings can be implicit (fields are automatically mapped to in fields with the same name in the index) or explicit (a mapping is defined to match a source field to an index field, often to rename the field to something more useful or to apply a function to the data value as it is mapped).
   - Output fields from the skills in the skillset are explicitly mapped from their hierarchical location in the output to the target field in the index.
 - Search an index
   - Full-text search: Full text search describes search solutions that parse text-based document contents to find query terms. Full text search queries in Azure AI Search are based on the Lucene query syntax, which provides a rich set of query operations for searching, filtering, and sorting data in indexes. Azure AI Search supports two variants of the Lucene syntax:
     - Simple - An intuitive syntax that makes it easy to perform basic searches that match literal query terms submitted by a user.
     - Full - An extended syntax that supports complex filtering, regular expressions, and other more sophisticated queries.
     - Client applications submit queries to Azure AI Search by specifying a search expression along with other parameters that determine how the expression is evaluated and the results returned. Some common parameters submitted with a query include:
       - search - A search expression that includes the terms to be found.
       - queryType - The Lucene syntax to be evaluated (simple or full).
       - searchFields - The index fields to be searched.
       - select - The fields to be included in the results.
       - searchMode - Criteria for including results based on multiple search terms. For example, suppose you search for `comfortable hotel`. A searchMode value of Any returns documents that contain "comfortable", "hotel", or both; while a searchMode value of All restricts results to documents that contain both "comfortable" and "hotel".
     - Query parsing. The search expression is evaluated and reconstructed as a tree of appropriate subqueries. Subqueries might include term queries, phrase queries, and prefix queries.
     - Lexical analysis: The query terms are analyzed and refined based on linguistic rules. For example, text is converted to lower case and nonessential stopwords (such as "the", "a", "is", and so on) are removed. Then words are converted to their root form (for example, "comfortable" might be simplified to "comfort") and composite words are split into their constituent terms.
     - Document retrieval: The query terms are matched against the indexed terms, and the set of matching documents is identified.
     - Scoring: A relevance score is assigned to each result based on a term frequency/inverse document frequency (TF/IDF) calculation.
 - Apply filtering and sorting
   - Filtering results
     - You can apply filters to queries in two ways:
       - By including filter criteria in a simple search expression.
       - By providing an OData filter expression as a $filter parameter with a full syntax search expression.
     - Filtering with facets: Facets are a useful way to present users with filtering criteria based on field values in a result set. They work best when a field has a small number of discrete values that can be displayed as links or options in the user interface. Facets are a useful way to present users with filtering criteria based on field values in a result set. They work best when a field has a small number of discrete values that can be displayed as links or options in the user interface.
   - Sorting results: By default, results are sorted based on the relevancy score assigned by the query process, with the highest scoring matches listed first. However, you can override this sort order by including an OData orderby parameter that specifies one or more sortable fields and a sort order (asc or desc).
 - Enhance the index
   - Search-as-you-type: By adding a `suggester` to an index, you can enable two forms of search-as-you-type experience to help users find relevant results more easily:
     - Suggestions - retrieve and display a list of suggested results as the user types into the search box, without needing to submit the search query.
     - Autocomplete - complete partially typed search terms based on values in index fields.
     - After you've added a suggester, you can use the suggestion and autocomplete REST API endpoints or SDK methods to submit a partial search term and retrieve a list of suggested results or autocompleted terms to display in the user interface. 
   - Custom scoring and result boosting: By default, search results are sorted by a relevance score that is calculated based on a term-frequency/inverse-document-frequency (TF/IDF) algorithm. You can customize the way this score is calculated by defining a **scoring profile** that applies a weighting value to specific fields - essentially increasing the search score for documents when the search term is found in those fields. Additionally, you can boost results based on field values - for example, increasing the relevancy score for documents based on how recently they were modified or their file size. After you've defined a scoring profile, you can specify its use in an individual search, or you can modify an index definition so that it uses your custom scoring profile by default.
   - Synonyms: Often, the same thing can be referred to in multiple ways. To help users find the information they need, you can define `synonym maps` that link related terms together. You can then apply those synonym maps to individual fields in an index, so that when a user searches for a particular term, documents with fields that contain the term or any of its synonyms will be included in the results.
## Create a custom skill for Azure AI Search
 - Create a custom skill: Your custom skill must implement the expected schema for input and output data that is expected by skills in an Azure AI Search skillset.
   - Input Schema: The input schema for a custom skill defines a JSON structure containing a record for each document to be processed. Each document has a unique identifier, and a data payload with one or more inputs
   - Output schema: The schema for the results returned by your custom skill reflects the input schema. It is assumed that the output contains a record for each input record, with either the results produced by the skill or details of any errors that occurred. The output value in this schema is a `property bag` that can contain any JSON structure, reflecting the fact that index fields aren't necessarily simple data values, but can contain complex types.
 - Add a custom skill to a skillset: To integrate a custom skill into your indexing solution, you must add a skill for it to a skillset using the Custom.WebApiSkill skill type.The skill definition must:
   - Specify the URI to your web API endpoint, including parameters and headers if necessary.
   - Set the context to specify at which point in the document hierarchy the skill should be called
   - Assign input values, usually from existing document fields
   - Store output in a new field, optionally specifying a target field name (otherwise the output name is used)
## Create a knowledge store with Azure AI Search
 - Knowledge stores: While the index might be considered the primary output from an indexing process, the enriched data it contains might also be useful in other ways. For example:
   - Since the index is essentially a collection of JSON objects, each representing an indexed record, it might be useful to export the objects as JSON files for integration into a data orchestration process using tools such as Azure Data Factory.
   - You may want to normalize the index records into a relational schema of tables for analysis and reporting with tools such as Microsoft Power BI.
   - Having extracted embedded images from documents during the indexing process, you might want to save those images as files.
   - Azure AI Search supports these scenarios by enabling you to define a `knowledge store` in the skillset that encapsulates your enrichment pipeline. The knowledge store consists of `projections` of the enriched data, which can be JSON objects, tables, or image files. When an indexer runs the pipeline to create or update an index, the projections are generated and persisted in the knowledge store.
 - Define projections: The projections of data to be stored in your knowledge store are based on the document structures generated by the enrichment pipeline in your indexing process. Each skill in your skillset iteratively builds a JSON representation of the enriched data for the documents being indexed, and you can persist some or all of the fields in the document as projections.
   - Using the `Shaper` skill: The process of indexing incrementally creates a complex document that contains the various output fields from the skills in the skillset. This can result in a schema that is difficult to work with, and which includes collections of primitive data values that don't map easily to well-formed JSON. To simplify the mapping of these field values to projections in a knowledge store, it's common to use the Shaper skill to create a new, field containing a simpler structure for the fields you want to map to projections.
 - Define a knowledge store: To define the knowledge store and the projections you want to create in it, you must create a `knowledgeStore` object in the skillset that specifies the Azure Storage connection string for the storage account where you want to create projections, and the definitions of the projections themselves. You can define object projections, table projections, and file projections depending on what you want to store; however note that you must define a `separate projection` for each type of projection, even though each projection contains lists for tables, objects, and files. Projection types are mutually exclusive in a projection definition, so only one of the projection type lists can be populated. If you create all three kinds of projection, you must include a projection for each type. For `object` and `file` projections, the specified container will be created if it does not already exist. An Azure Storage table will be created for each table projection, with the mapped fields and a unique key field with the name specified in the `generatedKeyName` property. These key fields can be used to define relational joins between the tables for analysis and reporting.
 - Summary:
   - Object projections are JSON representations of the indexed documents.
   - File projections are JPEG files containing image data extracted from documents.
   - Table projections create a relational schema for the extracted data.
## Enrich a search index using Language Studio
 - Explore the available features of Azure AI Language
   - Language Studio features:
     - Classify text
     - Understand questions and conversational language
     - Extract information
     - Summarize text
     - Translate text
     - Features can be either preconfigured or customizable. Preconfigured features can be tested straight away with a demo-like environment directly inside Language Studio. You can use them straight out of the box. The other features with * and green cogs in their logo need user customization. They require you to train their models so they fit your data better. After you have train them, you deploy and can then use them to power your apps or use the same demo-like testing environment.
   - Test and use preconfigured language features
   - Create, train, and deploy a conversation language understanding model
 - Enrich a cognitive search index with custom classes and Language Studio
   - Custom text classification allows you to map a passage of text to different user defined classes. For example, you could train a model on the synopsis on the back cover of books to automatically identify a books genre. You then use that identified genre to enrich your online shop search engine with a genre facet.
     - Store your documents so they can be accessed by Language Studio and Azure Cognitive Search indexers
     - Create a custom text classification project
     - Train and test your model
     - Create a search index based on your stored documents
     - Create a function app that uses your deployed trained model --> this is the custom skill which needs to be added to the skillset
       - There are five things the function app needs to know:
         - The text to be classified.
         - The endpoint for your trained custom text classification deployed model.
         - The primary key for the custom text classification project.
         - The project name.
         - The deployment name.
     - Update your search solution, your index, indexer, and custom skillset:
       - You need to add a field to your index to store the custom text classification enrichment.
       - You need to add a custom skillset to call your function app with the text to classify.
       - You need to map the response from the skillset into the index.

## Implement advanced search features in Azure Cognitive Search
 - Improve the ranking of a document with term boosting
   - Search an index: Azure Cognitive Search lets you query an index using a REST endpoint or inside the Azure portal with the search explorer tool. The querying processing consists of:
     - Query Parsing
       - Write a simple query
       - Enable the Lucene Query Parser: You can tell the search explorer to use the Lucene Query parser by adding `&queryType=full` to the query string. With the Lucene syntax, you can write more precise queries. Here is a summary of available features:
         - Boolean operators: AND, OR, NOT for example luxury AND 'air con'
         - Fielded search: fieldName:search term for example Description:luxury AND Tags:air con
         - Fuzzy search: ~ for example Description:luxury~ returns results with misspelled versions of luxury
         - Term proximity search: "term1 term2"~n for example "indoor swimming pool"~3 returns documents with the words indoor swimming pool within three words of each other
         - Regular expression search: /regular expression/ use a regular expression between / for example /[mh]otel/ would return documents with hotel and motel
         - Wildcard search: *, ? where * will match many characters and ? matches a single character for example 'air con'* would find air con and air conditioning
         - Precedence grouping: (term AND (term OR term)) for example (Description:luxury OR Category:luxury) AND Tags:air?con*
         - Term boosting: ^ for example Description:luxury OR Category:luxury^3 would give hotels with the category luxury a higher score than luxury in the description
       - Boost search terms: Using the above you can improve the results. The parser should give a higher priority to hotels in the luxury category. You can also be more precise and look for air conditioning in the Tags field. (eg. `^3` or `.*` are used to boost scores)
     - lexical Analysis
     - Document Retrieval
     - Scoring
 - Improve the relevance of results by adding scoring profiles
   - How search scores are calculated: Scoring is the last phase of processing a search query. The search engine scores the documents returned from the first three phases. The score is a function of the number of times identified search terms appear in a document, the document's size, and the rarity of each of the terms. By default, the search results are ordered by their search score, highest first. If two documents have an identical search score, you can break the tie by adding an $orderby clause.
   - Improve the score for more relevant documents: As the default scoring works on the frequency of terms and rarity, the final calculated score might not return the highest score for the most relevant document. Each dataset is different, so Cognitive Search lets you influence a document score using scoring profiles. The most straightforward scoring profile defines different weights for fields in an index. In the above example, the Hotel index has a scoring profile that has the Description field five times more relevant than data in the Location or Rooms fields. The Category field is twice as relevant as the HotelName. The scoring profile can also include functions, for example, distance or freshness. Functions provide more control than simple weighting, for example, you can define the boosting duration applied to newer documents before they score the same as older documents. The power of scoring profiles means that instead of boosting a specific term in a search request, you can apply a scoring profile to an index so that fields are boosted automatically for all queries.
   - Add a weighted scoring profile: You can add up to 100 scoring profiles to a search index. The simplest way to create a scoring profile is in the Azure portal. You can control which scoring profile is applied to a search query by appending the `&scoringProfile=PROFILE NAME` parameter.
     - Navigate to your search service.
     - Select Indexes, then select the index to add a scoring profile to.
     - Select Scoring profiles.
     - Select + Add scoring profile.
     - In Profile name, enter a unique name.
     - To set the scoring profile as a default to be applied to all searches select Set as default profile.
     - In Field name, select a field. Then for Weight, enter a weight value.
     - Select Save.
   - Use functions in a scoring profile
     - Magnitude:	Alter scores based on a range of values for a numeric field
     - Freshness:	Alter scores based on the freshness of documents as given by a DateTimeOffset field
     - Distance:	Alter scores based on the distance between a reference location and a GeographyPoint field
     - Tag:	Alter scores based on common tag values in documents and queries
 - Improve an index with analyzers and tokenized terms: Here, you'll learn how to define a custom analyzer to control how the content of a field is split into tokens for inclusion in the index.
   - Analyzers in Cognitive Search: When Cognitive Search indexes your content, it retrieves text. To build a useful index, with terms that help users locate documents, that text needs processing.
     - The text should be broken into words, often by using whitespace and punctuation characters as delimiters.
     - Stopwords, such as "the" and "it", should be removed because users don't search for them.
     - Words should be reduced to their root form.
     - If you don't specify an analyzer for a field, the default Lucene analyzer is used.
     - Built-in analyzers are of two types:
       - Language analyzers. If you need advanced capabilities for specific languages, such as lemmatization, word decompounding, and entity recognition, use a built-in language analyzer. Microsoft provides 50 analyzers for different languages.
       - Specialized analyzers. These analyzers are language-agnostic and used for specialized fields such as zip codes or product IDs. You can, for example, use the PatternAnalyzer and specify a regular expression to match token separators.
   - What is a custom analyzer? The built-in analyzers provide you with many options but sometimes you need an analyzer with unusual behavior for a field. In these cases, you can create a custom analyzer. A custom analyzer consists of:
     - Character filters. These filters process a string before it reaches the tokenizer. There are three character filters that you can use:
       - html_strip. This filter removes HTML constructs such as tags and attributes.
       - mapping. This filter enables you to specify mappings that replace one string with another. For example, you could specify a mapping that replaces TX with Texas.
       - pattern_replace. This filter enables you to specify a regular expression that identifies patterns in the input text and how matching text should be replaced.
     - Tokenizers. These components divide the text into tokens to be added to the index. Tokenizers also break down words into their root forms. Often, a token is a single word, but you might want to create unusual tokens such as: A full postal address. A complete URL or email address. there are 13 different tokenizers to choose from. These tokenizers include:
       - classic. This tokenizer processes text based on grammar for European languages.
       - keyword. This tokenizer emits the entire input as a single token. Use this tokenizer for fields that should always be indexed as one value.
       - lowercase. This tokenizer divides text at non-letters and then modifies the resulting tokens to all lower case.
       - microsoft_language_tokenizer. This tokenizer divides text based on the grammar of the language you specify.
       - pattern. This tokenizer divides texts where it matches a regular expression that you specify.
       - whitespace. This tokenizer divides text wherever there's white space.
     - Token filters. These filters remove or modify the tokens emitted by the tokenizer. There are forty one different token filters available, including:
       - Language-specific filters, such as arabic_normalization. These filters apply language-specific grammar rules to ensure that forms of words are removed and replaced with roots.
       - apostrophe. This filter removes any apostrophe from a token and any characters after the apostrophe.
       - classic. This filter removes English possessives and dots from acronyms.
       - keep. This filter removes any token that doesn't include one or more words from a list you specify.
       - length. This filter removes any token that is longer than your specified minimum or shorter than your specified maximum.
       - trim. This filter removes any leading and trailing white space from tokens.
   - Create a custom analyzer: You create a custom analyzer by specifying it when you define the index. You must do this with JSON code - there's no way to specify a custom index in the Azure portal. Use the analyzers section of the index at design time. You can include **only one** tokenizer but one or more character filters and one or more token filters.
   - Test a custom analyzer: Once you've defined your custom analyzer as part of your index, you can use the REST API's `Analyze Text` function to submit test text and ensure that the analyzer returns tokens correctly.
   - Use a custom analyzer for a field: Once you've defined and tested a custom analyzer, you can configure your index to use it. You can specify an analyzer for each field in your index. You can use the analyzer field when you want to use the same analyzer for both indexing and searching. It's also possible to use a different analyzer when indexing the field and when searching the field.
 - Enhance an index to include multiple languages: Support for multiple languages can be added to a search index. You can add language support manually by providing all the translated text fields in all the different languages you want to support. You could also choose to use Azure AI Services to provide translated text through an enrichment pipeline. Here, you'll see how to add fields with different languages to an index. You'll then constrain results to fields with specific languages. Finally, create a scoring profile to boost the native language of your end users.
   - Add language specific fields: To add multiple languages to an index, first, identify all the fields that need a translation. Then duplicate those fields for each language you want to support.
   - Limit the fields for a language: In this module, you've already seen how to limit the fields returned in a search request. You can also select which fields are being searched. Your language specific search solution can combine these two features to focus on fields with specific languages in them. Using the `searchFields` and `select` properties 
   - Enrich an index with multiple languages using Azure AI Services: If you don't have access to translations, you can enrich your index and add translated fields using Azure AI Services. For example, let's add Japanese and Ukrainian translations to an example retail properties index:
     - Add the new fields: You add two new fields to the index with these properties, the first to store the Japanese translation and the seconded the Ukrainian
     - Add the translation skillsets: You add two skills into the skillset definition to translate the document/description fields into the two languages.
     - Map the translated output into the index: The last step is to update the indexer to map the translated text into the index.
 - Improve search experience by ordering results by distance from a given reference point: Often, users want to search for items associated with a geographical location.
   - What are geo-spatial functions? To ask Cognitive Search to return results based on their location information, you can use two functions in your query:
     - `geo.distance`. This function returns the distance in a straight line across the Earth's surface from the point you specify to the location of the search result.
     - `geo.intersects`. This function returns `true` if the location of a search result is inside a polygon that you specify.
     - To use these functions, make sure that your index includes the location for results. Location fields should have the datatype `Edm.GeographyPoint` and store the `latitude` and `longitude`.
   - Use the geo.distance function: use filter declarative or order keyword `$filter=geo.distance(location, geography'POINT(-122.131577 47.678581)') le 5` or `orderby=geo.distance(Location, geography'POINT(2.294481 48.858370)') asc` : `Location`, `geography'POINT(2.294481 48.858370)'`, `le 5`, `asc`
   - Use the geo.intersects function: The geo.intersects function compares a location with a polygon on the Earth's surface, which you specify with three or more points. By using a polygon, you can create a shape that closely matches an area, such as an arrondissement. Use this polygon to add a geographical filter to your query: `geo.intersects(Location, geography'POLYGON((2.32 48.91, 2.27 48.91, 2.27 48.60, 2.32 48.60, 2.32 48.91))')`
     - geo.intersects returns a boolean value, so it's not possible to use it in an orderby clause.
     - In polygons, you must specify the points in counterclockwise order and the polygon must be closed, which means that the first and last points specified must be the same.
## Build an Azure Machine Learning custom skill for Azure Cognitive Search
 - Understand how to use a custom Azure Machine Learning skillset: Using a machine learning custom skill works the same as adding any other custom skill to a search index.
Here, you'll see how using the AmlSkill custom skill is different and explore the considerations of how to effectively use it.
   - Custom Azure Machine Learning skill schema: The best way to manage the efficiency of an AML skill is to scale up the Kubernetes inference cluster appropriately to manage your workload.
 - Enrich a search index using an Azure Machine Learning model: You create your Azure Machine Learning model using developer tools like the Python SDK, REST APIs, or Azure CLI. Another option is to take advantage of the Azure AI Machine Learning studio, a graphical user interface that lets you create, train, and deploy models without writing any code. General steps: Create Machine learning worksplace. --> Train model --> Edit scoring code --> Create endpoint --> Update cognitive search
   - Create an AML workspace: When you create the AML workspace, Azure will also create storage accounts, a key store, and application insights resources. The AML workspace Overview pane gives you a link to launch the Azure AI Machine Learning Studio.
   - Create and train a model in Azure Machine Learning studio: Azure AI Machine Learning Studio lets you use a designer to use drag and drop to create pipelines that create and train models. There's an even easier way to create models by using prebuilt templates.
   - Alter how the model works to allow it to be called by the AML custom skill: The models you train will normally use many examples of the data. The datasets will have many rows and be split and used to train and test the model. The code that handles this data and passes it to the model needs to be changed to handle single rows.
   - Create an endpoint for your model to use: The model is deployed to an endpoint. Azure AI Machine Learning Studio supports deploying a model to a real-time endpoint, a batch endpoint, or a web service. At the moment, **the custom `AmlSkill` skill in Azure Cognitive Search only supports web service endpoints**. The other restriction is that the endpoint has to be an Azure Kubernetes Service (AKS), container instances aren't supported. If you have experience in creating and managing AKS clusters, you can manually create the clusters in the Azure portal and reference them when you create your endpoint. However, an easier option is to let Azure AI Machine Learning Studio create and manage the cluster for you.
   - Connect the AML custom skill to the endpoint:
     - With everything above in place, you need to update your Azure Cognitive Search service. First, to enrich your search index you'll add a new field to your index to include the output for the model.
     - Then you'll update your index skillset and add the #Microsoft.Skills.Custom.AmlSkill custom skill.
     - Next, you'll change your indexer to map the output from the custom skill to the field you created on the index.
     - The last step is to rerun your indexer to enrich your index with the AML model.
## Search data outside the Azure platform in Azure Cognitive Search using Azure Data Factory
 - Index data from external data sources using Azure Data Factory:
   - Push data into a search index using Azure Data Factory (ADF): a zero-code option for pushing data into an index using ADF. ADF comes with connections to nearly 100 different data stores. With connectors like HTTP and REST that allow you to connect an unlimited number of data stores. These data stores are used as a source or a target (called sinks in the copy activity) in pipelines. The Azure Cognitive Search index connector can be used as a sink in a copy activity.
     - Create an ADF pipeline to push data into a search index
       - Create an Azure Cognitive Search index with all the fields you want to store data in.
       - Create a pipeline with a copy data step.
       - Create a data source connection to where your data resides.
       - Create a sink to connect to your search index.
       - Map the fields from your source data to your search index.
       - Run the pipeline to push the data into the index.
       - Limitations of using the built-in Azure Cognitive Search as a linked service: String, Int32, Int64, Double, Boolean, DataTimeOffset. This means ComplexTypes and arrays aren't currently supported. Looking at the JSON document above this means that it isn't possible to map all the phone numbers for the customer. Only the first telephone number has been mapped.
 - Index any data using the Azure Cognitive Search push API: The REST API is the most flexible way to push data into an Azure Cognitive Search index. You can use any programming language or interactively with any app that can post JSON requests to an endpoint.
   - Supported REST API operations: There are two supported REST APIs provided by cognitive search. Search and management APIs. This module focuses on the search REST APIs that provide operations on five features of search: **Index, Document, Indexer, Skillset, Synonym map**
   - How to call the search REST API: must use 'HTTPS' and must include **api-version** and **api-key**
     - Add data to an index: The body of your request needs to let the REST endpoint know the **action(upload, merge, mergeOrUpload, delete)** to take on the document, which document to apply the action too, and what data to use. You can add as many documents in the value array as you want. However, for optimal performance consider batching the documents in your requests up to a maximum of 1,000 documents, or 16 MB in total size.
   - Use .NET Core to index any data
     - How your index performs is based on six key factors:
       - The search service tier and how many replicas and partitions you've enabled.
       - The complexity of the index schema. Reduce how many properties (searchable, facetable, sortable) each field has.
       - The number of documents in each batch, the best size will depend on the index schema and the size of documents.
       - How multithreaded your approach is.
       - Handling errors and throttling. Use an exponential backoff retry strategy.
       - Where your data resides, try to index your data as close to your search index. For example, run uploads from inside the Azure environment.
     - Work out your optimal batch size
     - Implement an exponential backoff retry strategy
     - Use threading to improve performance
## Maintain an Azure Cognitive Search solution
 - Manage security of an Azure Cognitive Search solution
   - Overview of security approaches: ACS security builds on Azure's existing network security features. When you think about securing your search solution, you can focus on three areas:
     - Inbound search requests made by users to your search solution
     - Outbound requests from your search solution to other servers to index documents
     - Restricting access at the document level per user search request
   - Data encryption: The Azure Cognitive Search service, like all Azure services, encrypts the data it stores at rest with service-managed keys. This encryption includes indexes, data sources, synonym maps, skillsets, and even the indexer definitions. If you'd like to use your own encryption keys, ACS supports using the Azure Key Vault. A benefit of using your own customer-managed keys is that double encryption will be enabled on all objects you use your custom keys on.
   - Secure inbound traffic: If your search solution can be accessed externally from the internet or apps, you can reduce the attack surface. Azure Cognitive Search lets you restrict access to the public endpoint for free using a firewall to allow access from specific IP addresses. If your search service is only going to be used by on-premises resources, you can harden security with an ExpressRoute circuit, Azure Gateway, and an App service. There's also the option to change the public endpoint to use an Azure private link. You'll also need to set up an Azure virtual network and other resources. Using a private endpoint is the most secure solution, although it does come with the added cost of using those services that need to be hosted on the Azure platform.
     - Authenticate requests to your search solution: With the infrastructure in place to reduce the attack surface of your search solution, your focus can change to how to authenticate search requests from your users and apps. The default option when you create your ACS is key-based authentication. There are two different kinds of keys:
       - Admin keys - grant your write permissions and the right to query system information (maximum of 2 admin keys can be created per search service)
       - Query keys - grant read permissions and are used by your users or apps to query indexes (maximum of 50 query keys can be created per search service)
       - Role-based access control (RBAC) is provided by the Azure platform as a global system to control access to resources. You can use RBAC in Azure Cognitive Search in the following ways:
         - Roles can be granted access to administer the service
         - Define roles with access to create, load, and query indexes
         - The built-in roles you can assign to manage the Azure Cognitive Search service are:
           - Owner - Full access to all search resources
           - Contributor - Same as above, but without the ability to assign roles or change authorizations
           - Reader - View partial service information
         - If you need a role that can also manage the data plane for example search indexes or data sources, use one of these roles:
           - Search Service Contributor - A role for your search service administrators (the same access as the Contributor role above) and the content (indexes, indexers, data sources, and skillsets)
           - Search Index Data Contributor - A role for developers or index owners who will import, refresh, or query the documents collection of an index
           - Search Index Data Reader - Read-only access role for apps and users who only need to run queries
   - Secure outbound traffic: Typically your outbound traffic indexes source data or enriches it using Artificial Intelligence (AI). The outbound connections support using key-based authentication, database logins, or Microsoft Entra logins if you can use Microsoft Entra ID. If your data sources are hosted on the Azure platform, you can also secure connections using a system or user-assigned managed identity. Azure services can restrict access to them using a firewall. Your firewall can be configured to only allow the IP address of your Azure Cognitive Search service. If you're enriching your indexes with AI, you'll also need to allow all the IP addresses in the `AzureCognitiveSearch` service tag. You can choose to secure your source data behind a shared private link that your indexers use.
   - Secure data at the document-level: Controlling who has access at the document level requires you to update each document in your search index. You need to add a new security field to every document that contains the user or group IDs that can access it. The security field needs to be filterable so that you can filter search results on the field. With this field in place and populated with the allowed user or groups, you can restrict results by adding the `search.in` filter to all your search queries.
 - Optimize performance of an Azure Cognitive Search solution
   - Measure your current search performance: You can't optimize when you don't know how well your search service performs. Create a baseline performance benchmark so you can validate the improvements you make, but you can also check for any degradation in performance over time. To start with, enable diagnostic logging using Log Analytics. It's important to capture this diagnostic information at the search service level. As there are several places where your end-users or apps can see performance issues.
     - Check if your search service is throttled: Azure Cognitive Search searches and indexes can be throttled. If your users or apps are having their searches throttled, it's captured in Log Analytics with a 503 HTTP response. If your indexes are being throttled, they'll show up as 207 HTTP responses.
     - Check the performance of individual queries: The best way to test individual query performance is with a client tool like Postman. You can use any tool that will show you the headers in the response to a query. Azure Cognitive Search will always return an 'elapsed-time' value for how long it took the service to complete the query. If you want to know how long it would take to send and then receive the response from the client, subtract the elapsed time from the total round trip. In the above, that would be 125 ms - 21 ms giving you 104 ms.
   - Optimize your index size and schema: How your search queries perform is directly connected to the size and complexity of your indexes. The smaller and more optimized your indexes, the fast Azure Cognitive Search can respond to queries. Here are some tips that can help if you've found that you've performance issues on individual queries. If you don't pay attention, indexes can grow over time. You should review that all the documents in your index are still relevant and need to be searchable. If you can't remove any documents, can you reduce the complexity of the schema? Do you still need the same fields to be searchable? Do you still need all the skillsets you started the index with? Having too many attributes on a field limits its capabilities. If your index has been optimized but the performance still isn't where it needs to be, you can choose to scale up or scale out your search service.
   - Improve the performance of your queries: If you know how the search service works, you can tune your queries to drastically improve performance. Use this checklist for writing better queries:
     - Only specify the fields you need to search using the `searchFields` parameter. As more fields require extra processing.
     - Return the smallest number of fields you need to render on your search results page. Returning more data takes more time.
     - Try to avoid partial search terms like prefix search or regular expressions. These kinds of searches are more computationally expensive.
     - Avoid using high skip values. This forces the search engine to retrieve and rank larger volumes of data.
     - Limit using facetable and filterable fields to low cardinality data.
     - Use search functions instead of individual values in filter criteria. For example, you can use search.in(userid, '123,143,563,121',',') instead of $filter=userid eq 123 or userid eq 143 or userid eq 563 or userid eq 121.
     - If you've applied all of the above and still have individual queries that don't perform, you can scale out your index. Depending on the service tier you used to create your search solution, you can add up to 12 partitions. Partitions are the physical storage where your index resides. By default, all new search indexes are created with a single partition. If you add more partitions, the index is stored across them.
     - Adding extra partitions can help with performance as the search engine can run in parallel in each partition. The best improvements are seen for queries that return large numbers of documents and queries that use facets providing counts over large numbers of documents. This is a factor of how computationally expensive it's to score the relevancy of documents.
   - Use the best service tier for your search needs: You've seen that you can scale out service tiers by adding more partitions. You can scale out with replicas if you need to scale because of an increase in load. You can also scale up your search service by using a higher tier. You've seen that scaling out gives performance benefits due to parallelism. However, the higher tiers also come with premium storage, more powerful compute resources and extra memory.  Choosing the second option gives you more powerful infrastructure and allows for future index growth. Unfortunately which tier performs the best depends on the size and complexity of your index and the queries you write to search it. So either could be the best. Planning for future growth in the use of your search solution means you should consider search units. A search unit (SU) is the product of replicas and partitions. Think about needing to scale your search solution because of the increased load.
 - Manage costs of an Azure Cognitive Search solution: The costs of running an Azure Cognitive Search solution vary depending on the capacity and features you use.
   - Estimate your search solutions baseline costs: The Azure pricing calculator is a great tool that allows you to estimate the costs of using any of the Azure services. Use it to create a baseline for your search service needs. Part of running a cost-effective Azure Cognitive Search solution is always optimizing its capacity, from the tier you need, the data you're searching, and the features you use.
   - Understand the billing model
   - Tips to reduce the cost of your search solution:
     - Minimize bandwidth costs by using as few regions as possible. Ideally, all the resources should reside in the same region.
     - If you have predictable patterns of indexing new data, consider scaling up inside your search tier. Then scale back down for your regular querying.
     - To keep your search requests and responses inside the Azure datacenter boundary, use an Azure Web App front-end as your search app.
     - Enable enrichment caching if you're using AI enrichment on blob storage.
   - Manage search service costs using budgets and alerts: The most effective way to manage your costs is to monitor how much you're spending, and take action if the costs have increased over your budget. All Azure resources can be monitored with budgets in Microsoft Cost Management. With your budget in place, you can enable alerts to notify you if your organizations search stakeholders to avoid the risks of overspending.
 - Improve reliability of an Azure Cognitive Search solution
   - Make your search solution highly available:
     - The first and easiest way to improve the availability of your search solution is to increase the number of replicas. The only option is to have more than one in the paid-for search service tiers.
     - The second way to add redundancy to your search solution is to use the Availability Zones. This option requires that you use at least a standard tier.
   - Distribute your search solution globally: The most cost-efficient way to architect an Azure Cognitive Search service is in a single resource group and region. If your business priorities are availability and performance, host multiple versions of your search services in different geographical regions. The benefits of this architecture are:
     - Protection against failure in a region. Azure Cognitive Search doesn't support instant failover, you would need to handle it manually.
     - If you've globally distributed users or apps, locating a search service nearer to them will improve response times.
     - There's more work you'll need to do to replicate your indexes across all the regions you want to support. The options include having the same indexers based in each region ingesting the same source data. Or you can use the Push API to programmatically update all indexes in each region. The final piece is to manage search requests through an Azure Traffic Manager to route requests to the fastest responding search index (normally this will be the closest geographically unless that service isn't responding).
   - Back up options for your search indexes: At present, Azure doesn't offer a formal backup and restore mechanism for Azure Cognitive Search. However, you can build your own tools to back up index definitions as a series of JSON files. Then you can recreate your search indexes using these files.
 - Monitor an Azure Cognitive Search solution: Azure Monitor can give you insights into how well your search service is being used and performing. You can also receive alerts to proactively notify you of issues.
   - Monitor Azure Cognitive Search in Azure Monitor: When you create your Azure Cognitive Search service, without you doing any other setup, you can see your current search latency, queries per second, and the percentage of throttled queries. This data can be viewed on the `Monitoring` tab of the `Overview` page. You can also check what resources your search solution is using on the Usage tab. Once you have started using Log Analytics, you get access to performance and diagnostic data in these log tables: `AzureActivity - Shows you tasks that have been executed like scaling the search service`. `AzureDiagnostics - All the query and indexing operations`. `AzureMetrics - Data used for metrics that measure the health and performance of your search service`
     - Use metrics to see diagnostic data visually: Creating charts is a powerful way to view how your search service is performing. Under the Monitoring section of your search service, select Metrics. For example, you could plot search latency against the percentage of throttled queries to see if the responses to queries are affected by throttling.
     - Write Kusto queries against your search solutions logs: Log Analytics allows you to write any Kusto query against captured log data. The easiest way to run these queries is by selecting Logs under the Monitor section. Logs opens Log Analytics with the quest window automatically scoped to your Azure Cognitive Search solution. The following are useful queries to help you monitor and diagnose issues with your search solution:
       - Long-running queries
       - Indexer status
       - HTTP status codes
       - Query rates
       - Average Query Latency
       - Average Queries Per Minute (QPM)
       - Indexing Operations Per Minute (OPM)
   - Create alerts to be notified about common search solution issues: Alerts can let you proactively manage your search service. Here are some commonly used alerts you should consider creating:
     - Search Latency using the metric signal, you can specify what latency triggers the alert in seconds
     - Throttled search percentage using the metric signal, you can specify the percentage
     - Delete Search Service using the activity log signal, be notified if your search service is deleted
     - Stop Search Service using the activity log signal, be notified if your search service is stopped which happens if your search service is scaled up or down or needs to be restarted
 - Debug search issues using the Azure portal: When you first create your search service, you have to make some assumptions about the data you are indexing. You make choices about the index and how to ingest that data. However, until you run your created indexer you can't be certain that you made all the correct choices.
   - Explore how to use the Debug Session tool in Azure Cognitive Search: The Debug Session tool is an interactive visual editor that lets you step through the enrichment pipeline of a document as it's enriched. You can step into each individual skill, make changes and fixes, and then rerun the indexer in real-time. Once you've fixed any issues, you can update and republish the indexer so that it can be rerun to enrich all the documents in your index. After you've given your debug session a name, and chosen the index you'd like to debug, the search service copies everything it needs to an Azure Storage account. The copy includes the skillset, indexer, source data, and an enriched version of the document that is in the final index. The session is made up of a skill graph, enriched data source, skill detail pane, execution pane, and an errors/warnings pane. The skill detail pane allows you to expand an expression evaluator to check the value and test the inputs and outputs.
   - Debug a skillset with Debug Sessions: To create a Debug Session, you navigate to your search service in the Azure portal and carry out these steps:
     - Create a Debug Session
     - Explore and edit a skill
     - Validate the field mappings
## Get started with Azure OpenAI Service
## Build natural language solutions with Azure OpenAI Service
## Apply prompt engineering with Azure OpenAI Service


