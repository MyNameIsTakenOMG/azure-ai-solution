# AI-SOLUTION-REVIEW

## table of contents
 - [Get started with Azure AI Services](#get-started-with-azure-ai-services)
 - [Create computer vision solutions with Azure AI Vision](#create-computer-vision-solutions-with-azure-ai-vision)
 - [Develop natural language processing solutions with Azure AI Services](#develop-natural-language-processing-solutions-with-azure-ai-services)
 - [Implement knowledge mining with Azure AI Search](#implement-knowledge-mining-with-azure-ai-search)
 - [Develop solutions with Azure AI Document Intelligence](#develop-solutions-with-azure-ai-document-intelligence)
 - [Develop Generative AI solutions with Azure OpenAI Service](#develop-generative-ai-solutions-with-azure-openai-service)

## Get started with Azure AI Services

 - Prepare to develop AI solutions on Azure
   - Define artificial intelligence: AI as software that exhibits one or more human-like capabilities: `visual perception`,`text analysis and conversation`,`speech`,`decision making`
   - Understand AI-related terms:
     - **data science**: Data science is a discipline that focuses on the processing and analysis of data; applying statistical techniques to uncover and visualize relationships and patterns in the data, and defining experimental `models` that help explore those patterns.
     - **machine learning**: Machine learning is a subset of data science that deals with the training and validation of `predictive models`. Typically, a data scientist prepares the data and then uses it to train a model based on an algorithm that exploits the relationships between the `features` in the data to predict values for unknown `labels`.
     - **artificial intelligence**: Artificial intelligence usually (but not always) builds on machine learning to create software that emulates one or more characteristics of human intelligence.
   - Understand considerations for AI Engineers:
     - **model training and inferencing**: Many AI systems rely on predictive models that must be `trained` using sample data. The training process analyzes the data and determines relationships between the `features` in the data and the `label`. After the model has been trained, you can submit new data that includes known `feature` values and have the model predict the most likely `label`. Using the model to make predictions is referred to as `inferencing`. Many services and frameworks require the process of training a model using exsiting data before using ti to inference new values.
     - **probability and confidence scores**: A well-trained machine learning model can be accurate, but no predictive model is infallible. In most cases, predictions have an associated confidence score that reflects the probability on which the prediction is being made. Software developers should make use of confidence score values to evaluate predictions and apply appropriate thresholds to optimize application reliability and mitigate the risk of predictions that may be made based on marginal probabilities.
     - **responsible ai and ethics**: It's important for software engineers to consider the impact of their software on users, and society in general; including ethical considerations about its use. The human-like nature of AI solutions is a significant benefit in making applications user-friendly, but it can also lead users to place a great deal of trust in the application's ability to make correct decisions. The potential for harm to individuals or groups through incorrect predictions or misuse of AI capabilities is a major concern, and software engineers building AI-enabled solutions should apply due consideration to mitigate risks and ensure fairness, reliability, and adequate protection from harm or discrimination.
   - Understand considerations for responsible AI:
     - **fairness**: AI systems should treat all people fairly. (no bias based on gender, ethnicity or etc)
     - **reliability and safety**: AI systems should perform relibly and safely. Unreliability in these kinds of system can result in substantial risk to human life. As with any software, AI-based software application development must be subjected to rigorous testing and deployment management processes to ensure that they work as expected before release.
     - **privacy and security**: AI systems should be secure and respect privacy. Personal details must be kept private.
     - **inclusiveness**: AI systems should empower everyone and engage people regardless of physical ability, gender, sexual orientation, ethnicity, or other factors.
     - **transparency**: AI systems should be understandable. Users should be made fully aware of the purpose of the system, how it works, and what limitations may be expected.
     - **accountability**: People should be accountable for AI systems. Although many AI systems seem to operate autonomously, ultimately it's the responsibility of the developers who trained and validated the models they use, and defined the logic that bases decisions on model predictions to ensure that the overall system meets responsibility requirements.
   - Understand capabilities of Azure Machine Learning: a cloud-based platform for running experiments at scale to train predictive models from data, and publish the trained models as services. Features:
     - **Automated machine learning**
     - **Azure Machine Learning designer**
     - **Data and compute management**
     - **Pipelines**
   - Understand capabilities of Azure AI Services:
     - **natural language processing**: text analysis, QnA, translation, speech...
     - **knowledge mining and document intelligence**: ai search, document intelligence, custom document intelligence, custom skills...
     - **computer vision**: image analysis, face, video analysis, OCR...
     - **decision support**: content moderation...
     - **generative ai**: azure openai, dall-e image generation
   - Understand capabilities of Azure OpenAI Service: Generative AI models depend on large language models (LLMs) based on the transformer architecture that evolved from years of machine learning progress. Generative AI models are often queried with natural language prompts, and return an impressively accurate response when prompted correctly. `Azure OpenAI Service` is an Azure AI service for deploying, utilizing, and fine-tuning models developed by OpenAI. 
   - Understand capabilities of Azure AI Search: Azure AI Search is an Applied AI Service that enables you to ingest and index data from various sources, and search the index to find, filter, and sort information extracted from the source data. In addition to basic text-based indexing, Azure AI Search enables you to define an `enrichment pipeline` that uses AI skills to enhance the index with insights derived from the source data. Not only does this AI enrichment produce a more useful search experience, the insights extracted by your enrichment pipeline can be persisted in a knowledge store for further analysis or integration into a data pipeline for a business intelligence solution.
 - Create and consume Azure AI services: Azure AI services includes a wide range of individual services across multiple categories: `language`,`speech`,`vision`,`decision`. Different services can work together as a solution for certain scenario, such as `Azure AI document intelligence`,`Azure AI immersive reader`,`Azure cognitive search`,`Azure openai`
   - Create Azure AI services resources in an Azure subscription
     - multi-service resource and single-service resource(separated endpoints)
     - Training and prediction resources: some services offer separate resources for model training and prediction.
   - Identify endpoints, keys, and locations required to consume an Azure AI services resource.
     - `endpoint`: http address through which the service can be accessed.
     - `subscription key`: a valid key that must be provided when accessing endpoints
     - `resource location`: when provisioning a resource in Azure, this resource is assigned to a location, whhich determines the Azure data certer.
   - Use a REST API and an SDK to consume Azure AI services.
 - Secure Azure AI services
   - Consider authentication for Azure AI services
     - `regenerate subscription keys` (two keys for each ai service)
     - `protect keys with azure key vault`: Azure key vault can securely store secrets(passwords or keys). Access to key vault is granted to `security principals` which are authenticated user identities using `Microsoft Entra ID`. A security principal can be assigned to an application to define a `managed identity` for the application.
     - `token-based authentication`: When using the REST interface, some AI services support (or even require) token-based authentication. In these cases, the subscription key is presented in an initial request to obtain an authentication token, which has a valid period of 10 minutes. Subsequent requests must present the token to validate that the caller has been authenticated. (when using sdk, obtaining and presenting tokens are handled by sdk)
     - `Microsoft Entra ID authentication`: can be used to grant access to specific `service principals` or `managed identities` for apps and services.
       - `authenticate using service principals`
         - create a custom subdomain
         - assign a role to a service principal for you app
       - `authenticate using managed identities`
         - `system-assigned managed identity`: one-to-one, deleted when service deleted
         - `user-assigned managed identity`: reusable identity
   - Manage network security for Azure AI services: by default, azure ai services are accessible from all networks. But we can go to `networking/firewalls and virtual networks` to enable network restrictions.
 - Monitor Azure AI services
   - Monitor Azure AI services costs: using `azure pricing calculator` to create a new `estimate` and select an azure ai service you want to use (can choose multiple services). To view costs for the ai services, go to subscription --> cost analysis tab and view only costs for certain service by adding filters.
   - Create alerts and view metrics for Azure AI services
     - create alerts: create `alert rules` to configure notifications and alerts for your resources based on events or metric thresholds. `scope`: the resource to monitor; `condition`: on which the alert is triggered(`signal type`: `activity log` or `metric`); `optional actions`: sending emails or running azure logic aap to address the issue; `alert rule details`: such as the name of the alert rule and resource group where it should be defined.
     - view metrics: can add multiple metrics to a chart and choose appropriate aggregations and chart types.
     - add metrics to a dashboard
   - Manage Azure AI services diagnostic logging
     - create resources for diagnostic log storage: you can use Azure Event Hub as a destination so that then forward the data to a custom telemetry solution or 3rd party solution. But in most cases, you can use `Azure log analytics`, or `Azure storage`. **note**: create these resources before configuring diagnostic logging for your services
     - Configure diagnostic settings: go to `Diagnotic settings`, and specify the `name`, `catogories` of log event data to be captured, `details` of the destinations to store the data.
     - View log data in Azure Log Analytics
 - Deploy Azure AI services in containers
   - Create containers for reuse: pull container images from a registry and deploy it to a container host.
   - Deploy to a container and secure a container: download a contain image for specific azure ai service api and deploy it to ACI, or AKS or a local docker server. the usage metrics will be sent to azure ai services resource to calculate the billing.
     - configuration: `apikey`, `billing`,`eula`
     - containers:
       - language containers
       - speech containers
       - vision containers
   - Consume Azure AI services from a container: no need to provide subcription key, but has to implement your own authentication solution and network security restrictions.

## Create computer vision solutions with Azure AI Vision
 - Analyze images
   - Provision an Azure AI Vision resource
     - Description and tag generation - determining an appropriate caption for an image, and identifying relevant "tags" that can be used as keywords to indicate its subject.
     - Object detection - detecting the presence and location of specific objects within the image.
     - People detection - detecting the presence, location, and features of people in the image.
     - Image metadata, color, and type analysis - determining the format and size of an image, its dominant color palette, and whether it contains clip art.
     - Category identification - identifying an appropriate categorization for the image, and if it contains any known landmarks.
     - Background removal - detecting the background in an image and output the image with the background transparent or a greyscale alpha matte image.
     - Moderation rating - determine if the image includes any adult or violent content.
     - Optical character recognition - reading text in the image.
     - Smart thumbnail generation - identifying the main region of interest in the image to create a smaller "thumbnail" version.
   - Analyze an image:
     - sdk: create an `imageAnalysisClient` with `endpoint` and `key`; use `analyze` method with property `visual_features` assigned with `[VirsualFeatures]` (`VisualFeature enum`: TAGS, OBJECTS, CAPTION, DENSE_CAPTIONS,PEOPLE, SMART_CROPS,READ)
   - Generate a smart-cropped thumbnail and remove background:
     - thumbnail: specify `dimensions` and `region of interest` of the image
     - remove background: `alpha matte` of the foreground subject
 - Image classification with custom Azure AI Vision models
   - Create a custom Azure AI Vision classification model: (the types of custom models include `image classification`,`object detection`, and `product recognition`:more accuracy than `object detection`) first create an azure ai services resource or azure ai vision resource, then create a custom project.
     - Components of a custom Vision project: a `dataset` is the collection of images which is stored in an azure blob storage container, and a `COCO` file is the file that defines the labal info about those images. And the steps to take to train models:
       - create blob storage container and upload images
       - create dataset and connect it to the blob storage container, also define the type of project
       - label your data in azure ml data labeling project, which creates the COCO file in the blob storage container (each category need 3-5 images)
       - connect COCO file to the dataset
       - train custom model on dataset and labels
       - verify performance and iterate it if need
     - COCO files(json file):
       - images
       - annotations
       - categories
     - Creating your dataset
   - Understand image classification: classify the images based on their contents. models can be trained for multi-class or multi-label.
   - Understand object detection:
     - the class label of each object detected in the image
     - the location of each object within the image(bounding box)
   - Train an image classifier in Vision Studio
 - Detect, analyze, and recognize faces
   - Identify options for face detection, analysis, and identification.
     - Azure ai vision service: detect people and bounding box
     - face service: more comprehensive solution: face detection, facial feature analysis, face comparison and verification, facial recognition
   - Understand considerations for face analysis
     - data privacy and security
     - transparency
     - faireness and inclusiveness
   - Detect faces with the Computer Vision service: call `analyze iamge` function(sdk) or rest api requests, specifying `people` as `visual_feature`. Return the bounding box and basic attributes
   - Understand capabilities of the Face service
     - Face detection - for each detected face, the results include an ID that identifies the face and the bounding box coordinates indicating its location in the image.
     - Face attribute analysis - you can return a wide range of facial attributes, including:
       - Head pose (pitch, roll, and yaw orientation in 3D space)
       - Glasses (NoGlasses, ReadingGlasses, Sunglasses, or Swimming Goggles)
       - Blur (low, medium, or high)
       - Exposure (underExposure, goodExposure, or overExposure)
       - Noise (visual noise in the image)
       - Occlusion (objects obscuring the face)
       - Accessories (glasses, headwear, mask)
       - QualityForRecognition (low, medium, or high)
     - Facial landmark location - coordinates for key landmarks in relation to facial features (for example, eye corners, pupils, tip of nose, and so on)
     - Face comparison - you can compare faces across multiple images for similarity (to find individuals with similar facial features) and verification (to determine that a face in one image is the same person as a face in another image)
     - Facial recognition - you can train a model with a collection of faces belonging to specific individuals, and use the model to identify those people in new images.
     - Facial liveness - liveness can be used to determine if the input video is a real stream or a fake to prevent bad intentioned individuals from spoofing the recognition system.
   - Compare and match detected faces: when a face is detected, a unique id will be assigned to it and retained in the service resource for 24 hrs, and this id has no indication of this individual's identity other than their facial features, which can be used for facial comparison.
   - Implement facial recognition: (this feature is related to personal information, thus needs to apply for use)
     - create a `Person Group` that defines the sets of people you wanna identity
     - add `Person` to the `Person Group` for each person
     - add detected faces from multiple images to each `person`, the id for the person will persist (`persisted faces`)
     - lastly, train the model
     - the model can be used to identity individuals, verify identity of a detected face, analyze new images to find faces that are similar to a known one
 - Read Text in images and documents with the Azure AI Vision Service
   - options for reading texts:
     - image analysis OCR: used for general unstructured documents with small amount of text, or images that contain text; synchronous api call; has functionalities for analyzing images past exrtacting text
     - document intelligence: used for smail to large volumes of text from images and pdf documents; uses context and structure of the document to improve accuracy; asynchronous api call
   - Read text from images using OCR: use `imageAnalysis` function with `read` `visual_feature`. the return json results: `blocks`, `lines`,`text`/`words`
   - Use the Azure AI Vision service Image Analysis with SDKs and the REST API
   - Develop an application that can read printed and handwritten text
 - Analyze video
   - Describe Azure Video Indexer capabilities
     - Facial recognition - detecting the presence of individual people in the image. This requires Limited Access approval.
     - Optical character recognition - reading text in the video.
     - Speech transcription - creating a text transcript of spoken dialog in the video.
     - Topics - identification of key topics discussed in the video.
     - Sentiment - analysis of how positive or negative segments within the video are.
     - Labels - label tags that identify key objects or themes throughout the video.
     - Content moderation - detection of adult or violent themes in the video.
     - Scene segmentation - a breakdown of the video into its constituent scenes.
   - Extract custom insights: Azure Video Indexer includes predefined models that can recognize well-known celebrities, do OCR, and transcribe spoken phrases into text. You can extend the recognition capabilities of Video Analyzer by creating custom models for:
     - `people`: add images of the faces appear in the videos, and train the model to make video indexer recognize them
     - `language`: train a custom model to detect and transcribe specific terminology
     - `brands`; train a custom model to recognize names of brands, products,etc
   - Use Azure Video Indexer widgets and APIs: `Azure Video Indexer widgets` can be embedded into custom html interfaces, sharing insights from specific videos with others wihout giving people full access to your account in the azure video indexer portal. Or using `azure video indexer api` to create projects, retrive insights or modify models.
     - using `Azure resource manager` template to create azure ai video indexer resource 

## Develop natural language processing solutions with Azure AI Services
 - Analyze text with Azure AI Language
   - Detect language from text:
     - confidence score: 0 - 1
     - document size < 5120 characters
     - each collection limit: 1000, each item has a unique `id` and `text` (can also pass `countryHint` to improve prediction performance)
     - mixed language content returns the largest representation with a lower positive rateing
     - for the content where there is ambguity, response will be `unknown` with confidence score `0`
   - Analyze text sentiment: evaluate how positive and nagative the content is.
     - response contains overall sentiment and each sentence sentiment
     - sentence sentiment: `positive`, `negative`,`neutral`
     - all `neutral` sentences --> `neutral` document
     - only `positve` and `neutral` sentences --> `positive` document
     - only `negative` and `neutral` sentences --> `negative` document
     - `positive` and `negative` sentences --> `mixed` document
   - Extract key phrases, entities, and linked entities
     - extract key phrases: works best with larger documents(max: 5120 characters), can also pass multiple documents
     - entities: `person`, `location`,`datetime`,`organization`,`address`,`email`,`url`
     - linked entities: to disambiguate entities of the same name by referencing an article in a knowledge base.
 - Create question answering solutions with Azure AI Language
   - Understand question answering and how it compares to language understanding.
     - understand question answering: define a `knowledge base` of question and answer pairs that can be queried using natural language input. it can be published and  consumed by client apps, or bots. `knowledge base` is created from existing sources: web sites with `FAQ` documentation; files with structured text, like brochures or user guides; built-in chit char question and answer pairs having conversational exchanges.
     - question answering vs language undestanding
   - Create, test, publish, and consume a knowledge base.
     - create: `ai language` service, enable `question answering` feature; create `ai search` resource to host the index of the knowledge base; go to `language studio` to create `custom question answering` project; then add data sources to knowledge base; lastly edit question and answer pairs
     - test:
     - publish:
     - consume: through REST Api (`question`,`top`,`scoreThreshold`,`strictFilters`)
   - Implement multi-turn conversation and active learning:
     - multi-turn: ask clients follow-up questions to elicit more information from clients before presenting a definitive answer.
     - active learning: improve knowledge base by `active learning`(different questions with the same meanning, enabled by default) and defining `synonyms`(words with the same meaning)
   - Create a question answering bot to interact with using natural language.
 - Build a conversational language understanding model
   - Azure ai language service: pre-configured features and learned features
     - pre-configured features: `Summarization`,`Named entity recognition`,`Personally identifiable information (PII) detection`,`Key phrase extraction`,`Sentiment analysis`,`Language detection`.
     - learned features: `Conversational language understanding (CLU)`,`Custom named entity recognition`,`Custom text classification`,`Question answering`
   - Provision Azure resources for Azure AI Language resource
   - Define intents, utterances, and entities
     - utterances: phrases that a user might enter when interacting with an app that uses your language model.
     - intent: a task or action that a user wanna perform( the meaning of the utterance). Note: every model has a `None` intent that should be explicitly identify utterances with no action required or that falls outside of the scope of the domain of this model.
     - entities: used to add specific context to intents
       - learned entities
       - list entities
       - prebuilt entities
   - Use patterns to differentiate similar utterances: similar but with different meanings of utterances.
   - Use pre-built entity components
   - Train, test, publish, and review an Azure AI Language model
     - Train a model to learn intents and entities from sample utterances.
     - Test the model interactively or using a testing dataset with known labels
     - Deploy a trained model to a public endpoint so client apps can use it
     - Review predictions and iterate on utterances to train your model 
 - Create a custom text classification solution
   - Understand types of classification projects
     - single label classification
     - multiple label classification
     - labeling data
     - evaluating and improving your model: false positive, false negative
       - `recall`: Of all the actual labels, how many were identified; the ratio of true positives to all that was labeled.
       - `precision`: How many of the predicted labels are correct; the ratio of true positives to all identified positives.
       - `f1 scrore`: A function of recall and precision, intended to provide a single score to maximize for a balance of each component
   - Build a custom text classification project
     - define labels
     - tag data
     - train model
       - training set(80% recommended) and testing set
       - automatic split( great for large dataset) and manual spliy(best for smaller datasets)
     - view model
     - improve model(iterate)
     - deploy model
     - consume model
   - Tag data, train, and deploy a model
   - Submit classification tasks from your own app
 - Custom named entity recognition
   - Understand tagging entities in extraction projects
     - considerations for data selection and refining entities: high quality data(diversity, distribution, accuracy)
     - project limits:
       - training: > 10 files and < 100,000 files
       - 10 deployments per project
       - apis:
         - authoring: this api creates a project, trains and deploys your model. limited to 10 post and 100 get per min
         - analyze: this api extracts entities; requires a task and retrieves the results. limited to 20 get or post.
       - projects: 1 storage account per project; 500 projects per resource; 50 trained model per project
       - entities: 500 characters per entity, up to 200 entity types
     - label your data: (`consistency`,`percision`,`completeness`)
   - Understand how to build entity recognition projects
 - Translate text with Azure AI Translator service
   - Provision a Translator resource
   - Understand language detection, translation, and transliteration
   - Specify translation options
     - word alignment
     - sentence length
     - profanity filtering: `NoAction`,`Deleted`,`Marked`
   - Define custom translations
 - Create speech-enabled apps with Azure AI services
 - Translate speech with the Azure AI Speech service

## Implement knowledge mining with Azure AI Search

## Develop solutions with Azure AI Document Intelligence

## Develop Generative AI solutions with Azure OpenAI Service
